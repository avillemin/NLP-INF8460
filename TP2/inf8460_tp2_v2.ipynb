{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "DAZzAEuUBb5V",
    "outputId": "798a083b-de80-4a6f-b2a3-a2754cf59130"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JagJC4t2AnJ8"
   },
   "source": [
    "# √âcole Polytechnique de Montr√©al\n",
    "D√©partement G√©nie Informatique et G√©nie Logiciel\n",
    "INF8460 ‚Äì Traitement automatique de la langue naturelle\n",
    "\n",
    "### Prof. Amal Zouaq\n",
    "### Charg√© de laboratoire: F√©lix Martel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuOAuSwsAnJ-"
   },
   "source": [
    "# INF8460 - TP2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pq22DoZgAnJ_"
   },
   "source": [
    "## Objectifs\n",
    "\n",
    "‚Ä¢\tExplorer les mod√®les d‚Äôespaces vectoriels comme repr√©sentations distribu√©es de la s√©mantique des mots et des documents\n",
    "\n",
    "‚Ä¢\tComprendre diff√©rentes mesures de distance entre vecteurs de documents et de mots\n",
    "\n",
    "‚Ä¢\tUtiliser un mod√®le de langue n-gramme de caract√®res et l‚Äôalgorithme Naive Bayes pour l‚Äôanalyse de sentiments dans des revues de films (positives, n√©gatives)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sM-xQOHQAnKA"
   },
   "source": [
    "## 1. Pr√©traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvaRk_TMAnKB"
   },
   "source": [
    "Le jeu de donn√©es est s√©par√© en deux r√©pertoires `train/`et `test`, chacun contenant eux-m√™mes deux sous-r√©pertoires `pos/` et `neg/` pour les revues positives et n√©gatives. Un fichier `readme` d√©crit plus pr√©cis√©ment les donn√©es.\n",
    "\n",
    "Commencez par lire ces donn√©es, en gardant s√©par√©es les donn√©es d'entra√Ænement et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange, tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "unsugYpRAnKC",
    "outputId": "cec8d50b-6647-4f6b-dd20-4a540b551d97"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = './data/aclImdb/'\n",
    "\n",
    "def data_reader(path):\n",
    "    \"\"\" Cette fonction lit les donn√©es text et sauvegarde les labels. \n",
    "    \n",
    "    :param path : str, Chemin vers le dossier contenant les donn√©es.\n",
    "    :return X,y: list,list, X est une liste contenant les textes et y est une liste contenant les labels\"\"\"\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    for label in ['pos','neg']:\n",
    "        dir_path = path+label+'/'\n",
    "        for filename in os.listdir(dir_path) :\n",
    "            filepath = dir_path+filename\n",
    "            with open(filepath,'r', encoding = 'utf-8') as file_descriptor:\n",
    "                X.append(file_descriptor.read())\n",
    "                y.append(label)\n",
    "    return X,y\n",
    "\n",
    "X_train, y_train = data_reader(data_path+'train/')\n",
    "X_test, y_test = data_reader(data_path+'test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYEpFxJmAnKF"
   },
   "source": [
    "**a)** Cr√©ez la fonction `clean_doc()` qui effectue les pr√©-traitements suivants‚ÄØ: segmentation en mots‚ÄØ; \n",
    "suppression des signes de ponctuations‚ÄØ; suppression des mots qui contiennent des caract√®res autres qu‚Äôalphab√©tiques‚ÄØ; \n",
    "suppression des mots qui sont connus comme des stop words‚ÄØ; suppression des mots qui ont une longueur de 1 caract√®re. Ensuite, appliquez-la √† vos donn√©es.\n",
    "\n",
    "Les stop words peuvent √™tre obtenus avec `from nltk.corpus import stopwords`. Vous pourrez utiliser des [expressions r√©guli√®res](https://docs.python.org/3.7/howto/regex.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C3K598wKAnKG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bromwell', 'High', 'cartoon', 'comedy', 'It', 'ran', 'time', 'programs', 'school', 'life', 'Teachers', 'My', 'years', 'teaching', 'profession', 'lead', 'believe', 'Bromwell', 'High', 'satire', 'much', 'closer', 'reality', 'Teachers', 'The', 'scramble', 'survive', 'financially', 'insightful', 'students', 'see', 'right', 'pathetic', 'teachers', 'pomp', 'pettiness', 'whole', 'situation', 'remind', 'schools', 'knew', 'students', 'When', 'saw', 'episode', 'student', 'repeatedly', 'tried', 'burn', 'school', 'immediately', 'recalled', 'High', 'classic', 'line', 'INSPECTOR', 'sack', 'one', 'teachers', 'STUDENT', 'Welcome', 'Bromwell', 'High', 'expect', 'many', 'adults', 'age', 'think', 'Bromwell', 'High', 'far', 'fetched', 'What', 'pity']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-464657be11eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m#Affichage d'un test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m#Application de clean_doc √† l'ensemble du corpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-464657be11eb>\u001b[0m in \u001b[0;36mclean_doc\u001b[1;34m(document)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m#Segmentation en mots et suppression des signes de ponctuation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;31m#Suppression des stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\regexp.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;31m# If our regexp matches tokens, use re.findall:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_regexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "stop = stopwords.words(\"english\")\n",
    "reTokenizer = RegexpTokenizer('[A-Z|a-z]+')\n",
    "\n",
    "def clean_doc(document):\n",
    "    \"\"\"Effectue les pr√©-traitements suivants‚ÄØ √† un document : \n",
    "    segmentation en mots‚ÄØ; \n",
    "    suppression des signes de ponctuations‚ÄØ; \n",
    "    suppression des mots qui contiennent des caract√®res autres qu‚Äôalphab√©tiques‚ÄØ; \n",
    "    suppression des mots qui sont connus comme des stop words‚ÄØ; \n",
    "    suppression des mots qui ont une longueur de 1 caract√®re.\n",
    "    \n",
    "    :param sentence : str, le texte lu √† partir d'un fichier de donn√©es.\n",
    "    :return : list, liste de tokens trait√©s.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Segmentation en mots et suppression des signes de ponctuation\n",
    "    tokens = reTokenizer.tokenize(document)\n",
    "    #Suppression des stopwords\n",
    "    tokens = [token for token in tokens if token.lower() not in stop]\n",
    "    #Suppression des mots d'une longueur de 1 caract√®re\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    return tokens\n",
    "\n",
    "def clean_corpus(corpus):\n",
    "    \"\"\"Effectue les pr√©-traitements suivants‚ÄØ √† l'ensemble du corpus : \n",
    "    segmentation en mots‚ÄØ; \n",
    "    suppression des signes de ponctuations‚ÄØ; \n",
    "    suppression des mots qui contiennent des caract√®res autres qu‚Äôalphab√©tiques‚ÄØ; \n",
    "    suppression des mots qui sont connus comme des stop words‚ÄØ; \n",
    "    suppression des mots qui ont une longueur de 1 caract√®re.\n",
    "    \n",
    "    :param sentence : str, le texte lu √† partir d'un fichier de donn√©es.\n",
    "    :return : list, liste de tokens trait√©s.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [clean_doc(doc) for doc in corpus]\n",
    "    \n",
    "\n",
    "#Affichage d'un test\n",
    "print(X_train[0])\n",
    "print(clean_doc(X_train[0]))\n",
    "\n",
    "#Application de clean_doc √† l'ensemble du corpus\n",
    "X_train = clean_corpus(X_train)\n",
    "X_test = clean_corpus(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Pg5U3cXAnKL"
   },
   "source": [
    "**b)**\tCr√©ez la fonction `build_voc()` qui extrait les unigrammes de l‚Äôensemble d‚Äôentra√Ænement et conserve ceux qui ont une fr√©quence d‚Äôoccurrence de 5 au moins et imprime le nombre de mots dans le vocabulaire. Sauvegardez-le dans un fichier `vocab.txt` (un mot par ligne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnriS8dNAnKM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mots dans le vocabulaire: 33162\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm.vocabulary import Vocabulary\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "def build_voc(corpus):\n",
    "    \"\"\"\n",
    "    Extrait les unigrammes de l‚Äôensemble d‚Äôentra√Ænement et conserve ceux qui ont une fr√©quence d‚Äôoccurrence de 5 au moins\n",
    "    :param corpus: list(list(str)), un corpus tokeniz√©\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    ngrams, words = padded_everygram_pipeline(1, corpus)\n",
    "    vocab = Vocabulary(words, unk_cutoff=5)\n",
    "    print(\"Nombre de mots dans le vocabulaire:\",len(vocab)-1) \n",
    "    \n",
    "    with open('./output/vocab.txt',\"w\") as f:\n",
    "        for word in list(vocab)[:-1]:\n",
    "            f.write(word+'\\n')\n",
    "    f.close()\n",
    "    return None\n",
    "\n",
    "build_voc(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5yMNwiAAnKQ"
   },
   "source": [
    "**c)** Vous devez cr√©er une fonction `get_top_unigrams(n)` qui retourne les $n$ unigrammes les plus fr√©quents et les affiche, puis l'appeler avec $n=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n9I-NQ8YAnKR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('br', 101870), ('The', 45087), ('movie', 43362), ('film', 39692), ('one', 24414), ('like', 19503), ('It', 18433), ('This', 14935), ('good', 14496), ('time', 12438)]\n"
     ]
    }
   ],
   "source": [
    "def get_top_unigrams(corpus, n, unk_cutoff = 5):\n",
    "    \"\"\"\n",
    "    Retourne les  ùëõ  unigrammes les plus fr√©quents et les affiche\n",
    "    :param corpus: list(list(str)), un corpus tokeniz√©\n",
    "    :param n: int, nombre d'unigrammes √† afficher\n",
    "    :unk_cutoff: int, le seuil au-dessous duquel un mot est consid√©r√© comme inconnu et remplac√© par <UNK>\n",
    "    :return: list(tuple(str, int)), liste des top unigrams avec leur fr√©quence\n",
    "    \"\"\"\n",
    "    ngrams, words = padded_everygram_pipeline(1, corpus)\n",
    "    vocab = Vocabulary(words, unk_cutoff=unk_cutoff)\n",
    "    most_commun = vocab.counts.most_common(n)\n",
    "    print(most_commun)\n",
    "    return [unigram[0] for unigram in most_commun]\n",
    "\n",
    "most_commun_unigrams = get_top_unigrams(X_train,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1DaK2QfAnKW"
   },
   "source": [
    "**d)**\tVous devez cr√©er une fonction `get_top_unigrams_per_cls(n, cls)` qui retourne les $n$ unigrammes les plus fr√©quents de la classe `cls` (pos ou neg) et les affiche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVycuUqQAnKX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_top_unigrams_per_cls(n, cls, X, y):\n",
    "    \"\"\"\n",
    "    Retourne les  ùëõ  unigrammes les plus fr√©quents de la classe cls (pos ou neg) et les affiche\n",
    "    :param n: int, nombre d'unigrammes √† afficher\n",
    "    :param cls: str, classe √† rechercher (pos ou neg)\n",
    "    :param X: list(list(str)), un corpus tokeniz√©\n",
    "    :param y: list(str), liste des classes\n",
    "    :return: list(str), liste des top unigrams de la bonne classe\n",
    "    \"\"\"\n",
    "    X_cls = list(np.array(X)[np.array(y) == cls])\n",
    "    ngrams, words = padded_everygram_pipeline(1, X_cls)\n",
    "    vocab = Vocabulary(words, unk_cutoff=1)\n",
    "    most_commun = vocab.counts.most_common(n)\n",
    "    print(most_commun)\n",
    "    return [unigram[0] for unigram in most_commun]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dUh-CQQyAnKb"
   },
   "source": [
    "**e)**\tAffichez les 10 unigrammes les plus fr√©quents dans la classe positive :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGuo1CXfAnKc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('br', 49234), ('The', 22581), ('film', 20665), ('movie', 18816), ('one', 12361), ('It', 9625), ('like', 8628), ('This', 7675), ('good', 7346), ('story', 6517)]\n"
     ]
    }
   ],
   "source": [
    "top_unigrams_per_cls_pos = get_top_unigrams_per_cls(10, \"pos\", X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_k7_n0zAnKg"
   },
   "source": [
    "**f)**\tAffichez les 10 unigrammes les plus fr√©quents dans la classe n√©gative :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RqLKmy8TAnKh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('br', 52636), ('movie', 24546), ('The', 22506), ('film', 19027), ('one', 12053), ('like', 10875), ('It', 8808), ('This', 7260), ('good', 7150), ('bad', 7013)]\n"
     ]
    }
   ],
   "source": [
    "top_unigrams_per_cls_neg = get_top_unigrams_per_cls(10, \"neg\", X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8H1ioGiOAnKk"
   },
   "source": [
    "## 2. Matrices de co-occurence\n",
    "\n",
    "Pour les matrices de cette section, vous pourrez utiliser [des array `numpy`](https://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html) ou des DataFrame [`pandas`](https://pandas.pydata.org/pandas-docs/stable/). \n",
    "\n",
    "Ressources utiles :  le [*quickstart tutorial*](https://numpy.org/devdocs/user/quickstart.html) de numpy et le guide [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FMgl02sqAnKl"
   },
   "source": [
    "### 2.1 Matrice document √ó mot et TF-IDF\n",
    "\n",
    "\n",
    "Soit $X \\in \\mathbb{R}^{m \\times n}$ une matrice de $m$ documents et $n$ mots, telle que $X_{i,j}$ contient la fr√©quence d'occurrence du terme $j$ dans le document $i$ :\n",
    "\n",
    "$$\\textbf{rowsum}(X, d) = \\sum_{j=1}^{n}X_{dj}$$\n",
    "\n",
    "$$\\textbf{TF}(X, d, t) = \\frac{X_{d,t}}{\\textbf{rowsum}(X, d)}$$\n",
    "\n",
    "$$\\textbf{IDF}(X, t) = \\log\\left(\\frac{m}{|\\{d : X_{d,t} > 0\\}|}\\right)$$\n",
    "\n",
    "$$\\textbf{TF-IDF}(X, d, t) = \\textbf{TF}(X, d, t) \\cdot \\textbf{IDF}(X, t)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4a8M98Y8AnKn"
   },
   "source": [
    "En utilisant le m√™me vocabulaire de 5 000 unigrammes, vous devez repr√©senter les documents dans une matrice de co-occurrence document √ó mot $M(d, w)$  et les pond√©rer avec la mesure TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWLNxAFcnm-O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('br', 101870), ('The', 45087), ('movie', 43362), ('film', 39692), ('one', 24414), ('like', 19503), ('It', 18433), ('This', 14935), ('good', 14496), ('time', 12438), ('would', 12307), ('story', 11567), ('really', 11339), ('see', 11100), ('even', 11019), ('much', 9465), ('well', 9103), ('get', 9089), ('bad', 8825), ('people', 8791), ('great', 8404), ('made', 8253), ('first', 8072), ('make', 7884), ('way', 7796), ('also', 7776), ('could', 7773), ('movies', 7446), ('But', 7338), ('think', 7144), ('characters', 7049), ('character', 6960), ('And', 6799), ('films', 6758), ('seen', 6633), ('watch', 6451), ('plot', 6438), ('two', 6421), ('acting', 6304), ('life', 6265), ('many', 6264), ('There', 6205), ('never', 6152), ('know', 6075), ('little', 6025), ('show', 5991), ('In', 5964), ('love', 5904), ('best', 5859), ('ever', 5739), ('better', 5631), ('end', 5490), ('If', 5368), ('He', 5355), ('say', 5351), ('scene', 5343), ('scenes', 5161), ('still', 5080), ('man', 4937), ('something', 4885), ('go', 4809), ('back', 4763), ('real', 4532), ('thing', 4414), ('actors', 4413), ('years', 4393), ('watching', 4358), ('work', 4338), ('old', 4301), ('funny', 4172), ('makes', 4157), ('though', 4099), ('find', 4099), ('going', 4027), ('actually', 4006), ('look', 3915), ('lot', 3910), ('director', 3900), ('part', 3792), ('cast', 3787), ('nothing', 3766), ('another', 3702), ('want', 3639), ('quite', 3627), ('around', 3573), ('seems', 3568), ('pretty', 3522), ('fact', 3504), ('got', 3503), ('things', 3502), ('every', 3490), ('enough', 3418), ('thought', 3413), ('As', 3330), ('original', 3308), ('What', 3307), ('take', 3307), ('world', 3301), ('series', 3298), ('You', 3292), ('horror', 3281), ('long', 3278), ('young', 3266), ('action', 3254), ('us', 3233), ('They', 3212), ('give', 3208), ('gets', 3187), ('role', 3177), ('right', 3176), ('must', 3152), ('always', 3146), ('may', 3142), ('times', 3131), ('least', 3101), ('comedy', 3098), ('point', 3098), ('interesting', 3073), ('done', 3066), ('saw', 3063), ('new', 3062), ('whole', 3058), ('family', 3045), ('bit', 3043), ('without', 3031), ('script', 2995), ('almost', 2992), ('come', 2980), ('big', 2944), ('minutes', 2925), ('feel', 2913), ('making', 2900), ('performance', 2888), ('might', 2881), ('far', 2880), ('music', 2864), ('anything', 2861), ('guy', 2860), ('She', 2765), ('kind', 2719), ('TV', 2700), ('That', 2684), ('probably', 2683), ('away', 2679), ('last', 2647), ('So', 2647), ('woman', 2638), ('fun', 2636), ('girl', 2615), ('rather', 2604), ('since', 2573), ('found', 2571), ('played', 2561), ('hard', 2537), ('worst', 2533), ('course', 2486), ('screen', 2472), ('comes', 2462), ('When', 2459), ('trying', 2426), ('believe', 2422), ('goes', 2403), ('looking', 2400), ('day', 2376), ('looks', 2373), ('anyone', 2361), ('different', 2360), ('place', 2354), ('set', 2342), ('yet', 2338), ('especially', 2333), ('ending', 2321), ('book', 2321), ('put', 2320), ('DVD', 2315), ('reason', 2308), ('money', 2294), ('actor', 2288), ('shows', 2283), ('One', 2282), ('sense', 2281), ('sure', 2268), ('year', 2257), ('job', 2241), ('main', 2220), ('watched', 2219), ('plays', 2208), ('American', 2186), ('audience', 2183), ('together', 2183), ('takes', 2178), ('John', 2177), ('said', 2176), ('effects', 2175), ('play', 2174), ('seem', 2171), ('worth', 2165), ('someone', 2147), ('true', 2145), ('version', 2143), ('wife', 2114), ('My', 2112), ('beautiful', 2067), ('We', 2067), ('For', 2057), ('three', 2056), ('All', 2054), ('left', 2040), ('idea', 2028), ('half', 2022), ('Not', 2004), ('special', 2003), ('seeing', 1992), ('father', 1992), ('else', 1985), ('shot', 1983), ('However', 1977), ('later', 1975), ('everything', 1973), ('less', 1940), ('excellent', 1930), ('mind', 1921), ('everyone', 1909), ('nice', 1901), ('fan', 1890), ('high', 1886), ('read', 1885), ('simply', 1879), ('help', 1872), ('used', 1867), ('Hollywood', 1867), ('completely', 1857), ('budget', 1815), ('short', 1802), ('performances', 1799), ('let', 1793), ('either', 1777), ('rest', 1776), ('need', 1776), ('use', 1775), ('poor', 1761), ('second', 1756), ('line', 1755), ('low', 1750), ('try', 1745), ('classic', 1743), ('camera', 1742), ('given', 1738), ('kids', 1736), ('top', 1736), ('production', 1732), ('boring', 1728), ('home', 1726), ('wrong', 1721), ('enjoy', 1714), ('women', 1713), ('friends', 1691), ('start', 1690), ('tell', 1680), ('couple', 1671), ('mean', 1661), ('recommend', 1658), ('truly', 1654), ('moments', 1652), ('men', 1647), ('No', 1645), ('house', 1644), ('instead', 1640), ('came', 1639), ('understand', 1638), ('night', 1631), ('death', 1630), ('awful', 1627), ('stupid', 1617), ('Even', 1616), ('along', 1615), ('video', 1607), ('episode', 1601), ('stars', 1599), ('sex', 1597), ('small', 1595), ('playing', 1595), ('At', 1590), ('getting', 1588), ('full', 1586), ('person', 1581), ('remember', 1578), ('style', 1575), ('gives', 1572), ('After', 1568), ('To', 1564), ('although', 1559), ('Well', 1551), ('wonderful', 1550), ('however', 1547), ('often', 1546), ('face', 1545), ('become', 1543), ('next', 1538), ('lines', 1537), ('name', 1536), ('keep', 1535), ('written', 1532), ('terrible', 1531), ('school', 1522), ('others', 1521), ('black', 1520), ('felt', 1516), ('piece', 1516), ('dialogue', 1516), ('early', 1512), ('maybe', 1510), ('perfect', 1509), ('human', 1507), ('liked', 1505), ('supposed', 1502), ('star', 1489), ('head', 1478), ('THE', 1471), ('case', 1465), ('entire', 1446), ('went', 1445), ('sort', 1442), ('children', 1436), ('entertaining', 1427), ('waste', 1425), ('live', 1421), ('His', 1421), ('Mr', 1417), ('title', 1416), ('While', 1413), ('problem', 1412), ('called', 1405), ('war', 1404), ('Then', 1392), ('worse', 1392), ('absolutely', 1388), ('definitely', 1387), ('friend', 1386), ('mother', 1383), ('Don', 1376), ('beginning', 1374), ('becomes', 1373), ('Also', 1372), ('drama', 1366), ('boy', 1362), ('picture', 1361), ('already', 1360), ('cinema', 1360), ('seemed', 1358), ('certainly', 1355), ('laugh', 1355), ('lives', 1352), ('example', 1351), ('fans', 1342), ('care', 1332), ('wanted', 1332), ('Michael', 1329), ('loved', 1327), ('several', 1326), ('based', 1320), ('turn', 1319), ('dead', 1313), ('direction', 1312), ('lost', 1308), ('hope', 1306), ('Of', 1302), ('humor', 1298), ('fine', 1288), ('lead', 1287), ('Why', 1286), ('quality', 1283), ('wants', 1281), ('son', 1281), ('writing', 1274), ('works', 1272), ('tries', 1271), ('Some', 1263), ('past', 1255), ('guess', 1255), ('able', 1254), ('guys', 1253), ('viewer', 1250), ('How', 1249), ('killer', 1249), ('totally', 1249), ('Now', 1244), ('throughout', 1243), ('genre', 1243), ('flick', 1243), ('history', 1243), ('sound', 1236), ('enjoyed', 1235), ('heart', 1232), ('amazing', 1224), ('New', 1220), ('turns', 1220), ('side', 1217), ('finally', 1216), ('hand', 1213), ('evil', 1213), ('close', 1211), ('starts', 1210), ('gave', 1208), ('behind', 1207), ('etc', 1204), ('town', 1204), ('child', 1201), ('favorite', 1199), ('game', 1194), ('car', 1191), ('act', 1182), ('today', 1179), ('art', 1174), ('final', 1174), ('parts', 1172), ('late', 1169), ('self', 1167), ('days', 1163), ('expect', 1161), ('perhaps', 1158), ('white', 1158), ('actress', 1155), ('thinking', 1153), ('With', 1150), ('hour', 1150), ('stuff', 1148), ('stories', 1145), ('feeling', 1134), ('decent', 1132), ('voice', 1130), ('Just', 1125), ('directed', 1121), ('girls', 1113), ('brilliant', 1110), ('roles', 1109), ('type', 1109), ('daughter', 1109), ('horrible', 1108), ('matter', 1107), ('run', 1107), ('says', 1103), ('heard', 1101), ('slow', 1098), ('moment', 1097), ('killed', 1094), ('took', 1094), ('kid', 1090), ('eyes', 1089), ('fight', 1085), ('cannot', 1082), ('dark', 1079), ('involved', 1077), ('happens', 1076), ('kill', 1074), ('known', 1074), ('writer', 1062), ('hit', 1057), ('obvious', 1056), ('James', 1054), ('experience', 1052), ('violence', 1051), ('soon', 1050), ('happened', 1046), ('attempt', 1045), ('told', 1043), ('lack', 1042), ('including', 1038), ('happen', 1034), ('particularly', 1033), ('interest', 1032), ('ago', 1031), ('extremely', 1031), ('strong', 1028), ('leave', 1028), ('On', 1028), ('group', 1026), ('chance', 1024), ('David', 1023), ('stop', 1023), ('husband', 1017), ('complete', 1013), ('sometimes', 1011), ('except', 1009), ('coming', 1009), ('brother', 1004), ('looked', 1003), ('career', 1002), ('obviously', 1002), ('crap', 1001), ('score', 995), ('shown', 991), ('highly', 990), ('wonder', 986), ('age', 981), ('whose', 980), ('hero', 979), ('annoying', 977), ('Although', 977), ('number', 974), ('serious', 968), ('Oh', 967), ('taken', 966), ('alone', 966), ('relationship', 965), ('English', 965), ('musical', 965), ('jokes', 963), ('hours', 963), ('First', 960), ('simple', 959), ('started', 959), ('released', 958), ('exactly', 958), ('somewhat', 954), ('ends', 952), ('novel', 952), ('opinion', 950), ('level', 950), ('change', 949), ('possible', 949), ('sad', 948), ('opening', 947), ('reality', 946), ('Robert', 946), ('finds', 945), ('gore', 943), ('ridiculous', 940), ('usual', 937), ('across', 937), ('shots', 937), ('cinematography', 936), ('ones', 936), ('Man', 935), ('murder', 935), ('light', 934), ('hilarious', 934), ('view', 932), ('cut', 932), ('song', 928), ('talking', 927), ('female', 926), ('documentary', 926), ('despite', 924), ('saying', 923), ('body', 923), ('episodes', 923), ('talent', 922), ('turned', 922), ('running', 920), ('living', 918), ('save', 918), ('police', 915), ('important', 915), ('disappointed', 910), ('single', 901), ('taking', 899), ('events', 898), ('cool', 896), ('OK', 896), ('attention', 894), ('word', 894), ('huge', 893), ('wish', 891), ('British', 890), ('middle', 889), ('Paul', 888), ('modern', 887), ('usually', 887), ('order', 885), ('songs', 885), ('blood', 884), ('knew', 883), ('scary', 881), ('mostly', 881), ('non', 879), ('tells', 878), ('thriller', 877), ('Jack', 877), ('happy', 877), ('problems', 876), ('room', 875), ('comic', 875), ('sequence', 875), ('knows', 868), ('local', 868), ('call', 867), ('due', 865), ('silly', 864), ('future', 861), ('cheap', 859), ('television', 858), ('country', 857), ('easily', 853), ('George', 852), ('class', 847), ('sets', 842), ('words', 841), ('bring', 841), ('supporting', 840), ('Richard', 840), ('strange', 839), ('Oscar', 839), ('similar', 838), ('major', 838), ('appears', 838), ('predictable', 835), ('romantic', 833), ('clearly', 831), ('moving', 830), ('enjoyable', 830), ('seriously', 830), ('needs', 829), ('entertainment', 828), ('Maybe', 826), ('Unfortunately', 825), ('falls', 824), ('Yes', 823), ('giving', 818), ('review', 816), ('message', 816), ('From', 814), ('clich', 812), ('God', 811), ('fast', 811), ('mention', 809), ('sequel', 809), ('hell', 808), ('points', 807), ('four', 806), ('feels', 802), ('Lee', 801), ('stand', 798), ('ways', 797), ('theme', 797), ('York', 795), ('surprised', 795), ('five', 795), ('near', 793), ('release', 792), ('animation', 791), ('King', 790), ('effort', 790), ('none', 789), ('straight', 789), ('power', 787), ('storyline', 785), ('upon', 784), ('dull', 784), ('actual', 782), ('named', 781), ('within', 780), ('clear', 778), ('nearly', 778), ('talk', 777), ('working', 777), ('elements', 776), ('bunch', 774), ('minute', 774), ('overall', 773), ('beyond', 771), ('viewers', 769), ('begins', 768), ('tried', 767), ('team', 767), ('feature', 767), ('Peter', 762), ('form', 761), ('means', 761), ('dialog', 760), ('easy', 760), ('typical', 758), ('comments', 758), ('tale', 758), ('rating', 758), ('showing', 758), ('Her', 755), ('material', 755), ('Tom', 755), ('figure', 753), ('follow', 753), ('using', 751), ('realistic', 750), ('soundtrack', 750), ('famous', 749), ('editing', 748), ('sister', 748), ('Who', 746), ('kept', 745), ('hate', 745), ('fantastic', 744), ('fall', 743), ('doubt', 743), ('ten', 742), ('weak', 742), ('Disney', 742), ('theater', 741), ('leads', 740), ('certain', 738), ('period', 738), ('An', 738), ('viewing', 737), ('Like', 736), ('brought', 732), ('Its', 731), ('atmosphere', 730), ('particular', 728), ('whether', 728), ('th', 728), ('hear', 727), ('sequences', 725), ('parents', 724), ('eye', 722), ('America', 722), ('Is', 720), ('filmed', 719), ('among', 716), ('crime', 715), ('city', 715), ('reviews', 712), ('suspense', 711), ('stay', 710), ('Japanese', 710), ('learn', 709), ('These', 708), ('move', 707), ('premise', 707), ('apparently', 707), ('stage', 706), ('greatest', 706), ('believable', 705), ('sexual', 705), ('Most', 704), ('decided', 704), ('subject', 704), ('expected', 702), ('basically', 702), ('lame', 701), ('average', 697), ('buy', 697), ('French', 696), ('deal', 695), ('mystery', 693), ('became', 692), ('surprise', 690), ('sit', 689), ('difficult', 689), ('nature', 688), ('poorly', 688), ('By', 681), ('killing', 680), ('add', 678), ('leaves', 677), ('needed', 677), ('Dr', 676), ('yes', 675), ('question', 674), ('lots', 674), ('Joe', 673), ('begin', 673), ('Here', 673), ('credits', 668), ('gone', 667), ('situation', 664), ('meets', 662), ('NOT', 662), ('possibly', 659), ('somehow', 659), ('forced', 658), ('memorable', 657), ('Overall', 657), ('write', 657), ('free', 656), ('dramatic', 655), ('acted', 655), ('emotional', 654), ('dance', 654), ('realize', 654), ('earlier', 654), ('season', 654), ('screenplay', 653), ('reading', 652), ('older', 652), ('die', 651), ('male', 651), ('truth', 651), ('Jane', 651), ('superb', 650), ('laughs', 648), ('interested', 647), ('comment', 646), ('third', 645), ('footage', 644), ('writers', 643), ('forward', 642), ('directors', 642), ('keeps', 640), ('romance', 640), ('society', 638), ('anyway', 638), ('badly', 636), ('mess', 636), ('rent', 635), ('wait', 634), ('open', 633), ('War', 632), ('worked', 631), ('features', 631), ('imagine', 631), ('result', 630), ('perfectly', 630), ('quickly', 629), ('setting', 629), ('creepy', 627), ('unique', 627), ('effect', 627), ('please', 626), ('brings', 625), ('weird', 625), ('directing', 624), ('development', 624), ('forget', 623), ('meet', 622), ('previous', 621), ('admit', 620), ('appear', 619), ('ask', 619), ('rate', 617), ('joke', 617), ('personal', 614), ('background', 613), ('girlfriend', 613), ('Another', 612), ('business', 612), ('create', 610), ('cheesy', 609), ('meant', 609), ('powerful', 608), ('Christmas', 608), ('eventually', 607), ('fails', 606), ('general', 605), ('towards', 604), ('present', 604), ('check', 603), ('political', 602), ('potential', 602), ('fantasy', 600), ('leading', 599), ('hands', 599), ('portrayed', 598), ('masterpiece', 598), ('telling', 597), ('William', 595), ('Ben', 595), ('box', 595), ('deep', 595), ('villain', 594), ('incredibly', 594), ('various', 593), ('sorry', 593), ('Or', 592), ('ideas', 592), ('reasons', 592), ('IMDb', 591), ('fighting', 590), ('note', 590), ('casting', 588), ('era', 588), ('shame', 588), ('hot', 587), ('pay', 586), ('sounds', 586), ('deserves', 585), ('break', 584), ('front', 582), ('manages', 582), ('expecting', 581), ('indeed', 581), ('Scott', 581), ('plenty', 579), ('attempts', 578), ('battle', 578), ('remake', 578), ('missing', 577), ('crazy', 577), ('success', 576), ('space', 576), ('talented', 575), ('beauty', 574), ('total', 574), ('twist', 574), ('miss', 573), ('cop', 573), ('outside', 572), ('copy', 572), ('fairly', 571), ('agree', 571), ('nudity', 570), ('whatever', 568), ('flat', 567), ('wrote', 565), ('Good', 565), ('dumb', 564), ('Great', 564), ('inside', 564), ('mentioned', 564), ('gay', 563), ('social', 563), ('married', 563), ('air', 563), ('hardly', 562), ('cute', 559), ('monster', 558), ('wasted', 555), ('plain', 554), ('recently', 554), ('Mary', 554), ('dream', 553), ('filmmakers', 552), ('Instead', 548), ('ended', 548), ('crew', 547), ('missed', 547), ('Director', 546), ('decides', 546), ('Let', 546), ('caught', 546), ('members', 545), ('Bill', 544), ('co', 542), ('list', 542), ('pace', 542), ('popular', 540), ('created', 540), ('large', 540), ('waiting', 539), ('lady', 538), ('Star', 538), ('uses', 538), ('convincing', 537), ('match', 536), ('sees', 536), ('produced', 536), ('spent', 535), ('US', 535), ('familiar', 534), ('slightly', 534), ('hold', 531), ('rich', 529), ('odd', 529), ('unless', 529), ('filled', 528), ('tension', 527), ('entirely', 527), ('public', 527), ('dog', 527), ('unfortunately', 526), ('Perhaps', 525), ('moves', 525), ('bored', 524), ('Best', 522), ('laughing', 522), ('value', 521), ('successful', 520), ('choice', 520), ('boys', 520), ('concept', 519), ('intelligent', 519), ('zombie', 519), ('pure', 518), ('Do', 518), ('credit', 518), ('dancing', 518), ('clever', 516), ('Italian', 516), ('Movie', 515), ('positive', 514), ('cover', 513), ('World', 513), ('cartoon', 512), ('Tony', 512), ('store', 511), ('visual', 511), ('language', 510), ('speak', 510), ('kills', 510), ('following', 509), ('depth', 509), ('effective', 509), ('portrayal', 508), ('recent', 507), ('appreciate', 507), ('House', 506), ('focus', 506), ('exciting', 506), ('biggest', 505), ('German', 503), ('apart', 502), ('escape', 502), ('Dead', 502), ('common', 501), ('amusing', 501), ('runs', 500), ('control', 500), ('died', 499), ('former', 498), ('incredible', 498), ('fire', 498), ('spend', 498), ('singing', 498), ('Night', 496), ('follows', 496), ('violent', 496), ('impressive', 495), ('younger', 495), ('amount', 494), ('Still', 494), ('water', 494), ('Black', 493), ('books', 492), ('sweet', 492), ('compared', 491), ('bizarre', 490), ('adult', 490), ('respect', 490), ('cause', 489), ('office', 489), ('win', 488), ('makers', 488), ('revenge', 488), ('avoid', 487), ('showed', 487), ('pointless', 487), ('otherwise', 486), ('chemistry', 485), ('gun', 485), ('fi', 484), ('solid', 484), ('culture', 484), ('trouble', 484), ('fit', 483), ('situations', 483), ('changed', 483), ('accent', 483), ('Smith', 482), ('Jim', 482), ('considered', 481), ('decide', 481), ('hair', 481), ('Steve', 481), ('suddenly', 480), ('secret', 479), ('trash', 479), ('failed', 478), ('disturbing', 477), ('animated', 477), ('party', 477), ('Anyway', 476), ('slasher', 476), ('longer', 475), ('baby', 475), ('studio', 474), ('questions', 473), ('consider', 472), ('barely', 472), ('producers', 472), ('shooting', 470), ('trip', 470), ('walk', 470), ('adventure', 469), ('honest', 469), ('conclusion', 468), ('sick', 467), ('Very', 466), ('fake', 466), ('values', 465), ('likes', 465), ('Though', 465), ('City', 464), ('audiences', 464), ('impossible', 464), ('involving', 464), ('rock', 464), ('Big', 463), ('Watch', 462), ('tough', 462), ('Stewart', 462), ('leaving', 461), ('band', 461), ('sci', 460), ('heavy', 460), ('meaning', 460), ('state', 460), ('doctor', 460), ('college', 460), ('People', 459), ('London', 459), ('images', 458), ('earth', 458), ('charming', 457), ('cult', 456), ('rated', 455), ('awesome', 454), ('ability', 454), ('bought', 454), ('computer', 453), ('starring', 453), ('anti', 453), ('Only', 452), ('return', 451), ('company', 451), ('Where', 450), ('appearance', 449), ('literally', 448), ('normal', 448), ('garbage', 448), ('explain', 448), ('pathetic', 447), ('appeal', 447), ('Earth', 447), ('Sam', 447), ('Harry', 447), ('immediately', 446), ('week', 446), ('Every', 446), ('oh', 446), ('touch', 445), ('adaptation', 445), ('cold', 445), ('prison', 445), ('basic', 444), ('Two', 444), ('aspect', 442), ('genius', 442), ('Because', 441), ('Despite', 441), ('ultimately', 440), ('standard', 440), ('Love', 440), ('somewhere', 440), ('stick', 439), ('post', 439), ('purpose', 439), ('rare', 438), ('utterly', 438), ('red', 437), ('considering', 436), ('Charlie', 436), ('fear', 435), ('humour', 435), ('glad', 434), ('neither', 434), ('thinks', 433), ('shoot', 432), ('twists', 431), ('terms', 431), ('sitting', 431), ('loud', 431), ('project', 430), ('magic', 430), ('Kelly', 430), ('ex', 429), ('comedies', 429), ('drawn', 428), ('military', 428), ('sexy', 428), ('spirit', 427), ('generally', 426), ('mood', 425), ('pick', 425), ('zombies', 425), ('street', 424), ('added', 424), ('touching', 424), ('managed', 424), ('subtle', 424), ('OF', 423), ('motion', 423), ('unbelievable', 423), ('natural', 423), ('okay', 422), ('tone', 422), ('catch', 421), ('narrative', 421), ('likely', 421), ('Nothing', 421), ('nowhere', 420), ('climax', 419), ('force', 419), ('fully', 419), ('themes', 419), ('plus', 418), ('Film', 418), ('Frank', 418), ('terrific', 417), ('taste', 417), ('complex', 417), ('Sure', 416), ('issues', 416), ('Chris', 416), ('remains', 415), ('laughable', 415), ('century', 414), ('mistake', 414), ('constantly', 413), ('surprisingly', 413), ('science', 412), ('victim', 412), ('door', 412), ('date', 411), ('finish', 411), ('presented', 411), ('key', 410), ('presence', 410), ('equally', 410), ('manner', 410), ('Christopher', 410), ('costumes', 409), ('Yet', 409), ('fair', 408), ('pieces', 407), ('beautifully', 407), ('excuse', 407), ('scenery', 407), ('drive', 407), ('painful', 407), ('innocent', 406), ('Once', 406), ('impression', 405), ('Paris', 405), ('pass', 405), ('disappointing', 404), ('marriage', 404), ('developed', 404), ('knowing', 404), ('brain', 404), ('thrown', 403), ('cinematic', 403), ('details', 403), ('alive', 402), ('Charles', 402), ('Allen', 401), ('historical', 401), ('places', 401), ('exception', 400), ('disappointment', 400), ('animals', 400), ('super', 399), ('dreams', 399), ('hoping', 399), ('numbers', 398), ('charm', 398), ('Henry', 398), ('unlike', 397), ('photography', 397), ('aspects', 397), ('stands', 396), ('expectations', 395), ('slowly', 395), ('feelings', 394), ('outstanding', 394), ('suppose', 393), ('stunning', 393), ('Rock', 393), ('sent', 391), ('mysterious', 391), ('boyfriend', 391), ('emotion', 391), ('recommended', 391), ('Bruce', 391), ('Jones', 391), ('Batman', 391), ('drug', 390), ('held', 389), ('lovely', 389), ('opportunity', 389), ('track', 389), ('Indian', 389), ('names', 387), ('fiction', 387), ('filming', 387), ('support', 387), ('walking', 387), ('noir', 387), ('bother', 386), ('intended', 386), ('Will', 386), ('Young', 385), ('building', 385), ('soldiers', 385), ('contains', 385), ('Many', 384), ('cat', 384), ('Little', 383), ('edge', 383), ('likable', 383), ('emotions', 383), ('acts', 383), ('producer', 383), ('South', 383), ('happening', 381), ('compelling', 381), ('silent', 381), ('attack', 381), ('government', 381), ('soul', 381), ('pictures', 380), ('lived', 380), ('gang', 380), ('ride', 379), ('changes', 379), ('brief', 378), ('puts', 378), ('loves', 377), ('dies', 377), ('fascinating', 377), ('available', 377), ('bar', 377), ('suggest', 377), ('lover', 376), ('Story', 376), ('detective', 376), ('tired', 376), ('include', 375), ('smart', 375), ('element', 375), ('difference', 374), ('De', 374), ('critics', 373), ('bed', 373), ('pain', 373), ('victims', 373), ('camp', 373), ('mainly', 372), ('offer', 372), ('Other', 371), ('minor', 370), ('spot', 370), ('journey', 370), ('law', 370), ('serial', 369), ('appeared', 369), ('approach', 369), ('Christian', 369), ('dad', 369), ('fellow', 368), ('western', 368), ('adults', 367), ('actresses', 366), ('train', 366), ('student', 365), ('image', 365), ('fresh', 365), ('II', 365), ('Jerry', 365), ('share', 364), ('lacks', 364), ('mad', 363), ('confused', 363), ('Martin', 363), ('falling', 363), ('event', 362), ('Billy', 362), ('Both', 362), ('followed', 362), ('god', 362), ('flaws', 361), ('Americans', 361), ('West', 361), ('trailer', 361), ('mix', 360), ('Red', 360), ('helps', 359), ('relationships', 359), ('system', 359), ('confusing', 359), ('Arthur', 359), ('content', 358), ('shock', 357), ('rape', 357), ('road', 357), ('Please', 357), ('lighting', 356), ('worthy', 356), ('moral', 356), ('ahead', 356), ('proves', 356), ('imagination', 355), ('flicks', 355), ('naked', 355), ('wondering', 354), ('mediocre', 354), ('porn', 354), ('standards', 354), ('sub', 354), ('merely', 354), ('laughed', 354), ('students', 353), ('color', 353), ('Davis', 353), ('brothers', 353), ('delivers', 353), ('tragedy', 352), ('direct', 352), ('latter', 352), ('boss', 351), ('gorgeous', 351), ('forever', 351), ('gem', 351), ('creative', 351), ('Brian', 351), ('childhood', 350), ('forgotten', 350), ('paid', 350), ('negative', 350), ('provides', 350), ('attractive', 350), ('nobody', 350), ('aside', 349), ('impressed', 349), ('throw', 349), ('random', 348), ('chase', 348), ('Alan', 348), ('notice', 346), ('offers', 346), ('IS', 346), ('addition', 346), ('justice', 346), ('step', 345), ('million', 345), ('murders', 345), ('putting', 345), ('surely', 344), ('Having', 344), ('tragic', 344), ('Too', 344), ('opera', 344), ('Williams', 343), ('inspired', 343), ('thoroughly', 343), ('becoming', 343), ('winning', 343), ('funniest', 343), ('thanks', 342), ('sadly', 342), ('island', 342), ('Van', 342), ('Jean', 342), ('answer', 341), ('central', 341), ('fashion', 340), ('detail', 340), ('stuck', 340), ('pull', 339), ('reminded', 339), ('ghost', 339), ('twice', 339), ('fell', 339), ('double', 339), ('Ms', 338), ('ship', 338), ('seemingly', 338), ('honestly', 338), ('Bad', 338), ('wall', 337), ('adds', 336), ('Jackson', 336), ('Day', 336), ('wise', 335), ('de', 335), ('plan', 334), ('personality', 334), ('apartment', 334), ('planet', 334), ('race', 334), ('ground', 334), ('information', 333), ('intense', 333), ('afraid', 333), ('location', 333), ('length', 333), ('turning', 333), ('industry', 333), ('Since', 333), ('supposedly', 332), ('folks', 332), ('affair', 332), ('Mark', 332), ('Life', 331), ('ready', 331), ('six', 331), ('giant', 330), ('artistic', 330), ('rented', 330), ('seconds', 330), ('shocking', 330), ('asks', 329), ('AND', 329), ('describe', 329), ('filmmaker', 329), ('Danny', 329), ('artist', 328), ('anymore', 328), ('area', 328), ('design', 328), ('hospital', 327), ('onto', 327), ('picked', 327), ('Chinese', 327), ('absolute', 327), ('deliver', 327), ('speaking', 326), ('quick', 326), ('nasty', 326), ('faces', 326), ('extreme', 326), ('lose', 325), ('struggle', 325), ('Everything', 325), ('finding', 325), ('scientist', 325), ('redeeming', 325), ('Can', 324), ('helped', 323), ('cry', 323), ('professional', 323), ('member', 323), ('impact', 322), ('holes', 322), ('led', 322), ('collection', 322), ('necessary', 321), ('clothes', 321), ('allowed', 321), ('wearing', 321), ('White', 321), ('Stephen', 321), ('moved', 320), ('Jason', 320), ('began', 319), ('Their', 319), ('drugs', 319), ('whatsoever', 318), ('Ray', 318), ('Which', 317), ('build', 317), ('willing', 317), ('includes', 317), ('damn', 316), ('bottom', 316), ('realized', 316), ('favourite', 316), ('mouth', 316), ('flying', 315), ('angry', 315), ('beat', 315), ('hotel', 315), ('master', 314), ('Before', 314), ('sight', 314), ('teen', 314), ('deeply', 314), ('energy', 314), ('intelligence', 314), ('everybody', 314), ('wonderfully', 313), ('Taylor', 313), ('Al', 313), ('sleep', 313), ('wooden', 313), ('vampire', 313), ('comedic', 312), ('born', 312), ('mid', 312), ('Douglas', 312), ('introduced', 311), ('accident', 311), ('loving', 311), ('actions', 310), ('phone', 310), ('ugly', 310), ('suicide', 310), ('Andy', 310), ('Finally', 309), ('apparent', 309), ('tears', 309), ('thus', 309), ('mark', 309), ('epic', 308), ('thin', 308), ('carry', 307), ('disaster', 307), ('martial', 307), ('Keaton', 307), ('dying', 306), ('See', 306), ('extra', 306), ('superior', 306), ('teacher', 306), ('plane', 306), ('food', 306), ('Eddie', 306), ('engaging', 305), ('land', 305), ('listen', 305), ('Go', 305), ('provide', 305), ('continue', 305), ('Johnny', 305), ('commentary', 305), ('Did', 305), ('Those', 304), ('agent', 303), ('religious', 303), ('humans', 303), ('remarkable', 303), ('physical', 302), ('mental', 302), ('Tim', 302), ('pleasure', 302), ('unnecessary', 301), ('unusual', 301), ('personally', 301), ('anywhere', 301), ('process', 301), ('Hitler', 301), ('Joan', 301), ('paced', 300), ('memory', 300), ('accept', 300), ('scared', 300), ('station', 300), ('finished', 300), ('Miss', 299), ('Kate', 299), ('technical', 299), ('Jeff', 299), ('Russian', 299), ('Fred', 299), ('exist', 298), ('Adam', 298), ('teenage', 298), ('lets', 297), ('surprising', 297), ('watchable', 297), ('Ford', 297), ('limited', 296), ('Horror', 296), ('absurd', 296), ('pop', 295), ('mom', 295), ('wild', 295), ('holds', 295), ('CGI', 295), ('allow', 295), ('compare', 295), ('news', 294), ('brutal', 294), ('bits', 294), ('creature', 294), ('Ed', 294), ('England', 294), ('desperate', 293), ('suspect', 293), ('criminal', 293), ('independent', 293), ('asked', 292), ('vision', 292), ('woods', 292), ('Everyone', 291), ('intriguing', 291), ('wanting', 291), ('treat', 291), ('soft', 291), ('Jr', 291), ('Shakespeare', 291), ('pilot', 291), ('Okay', 290), ('Was', 289), ('heroes', 289), ('rip', 289), ('instance', 289), ('rarely', 288), ('search', 288), ('club', 288), ('sat', 288), ('player', 288), ('issue', 287), ('pacing', 287), ('reminds', 287), ('smile', 287), ('deserved', 286), ('heroine', 286), ('grade', 286), ('deserve', 285), ('jump', 285), ('count', 285), ('gotten', 285), ('capture', 285), ('Japan', 285), ('arts', 285), ('thats', 284), ('accurate', 284), ('explained', 284), ('media', 284), ('Ann', 283), ('players', 283), ('explanation', 283), ('plots', 283), ('Jesus', 283), ('heads', 283), ('knowledge', 282), ('lovers', 282), ('creating', 282), ('cops', 282), ('hated', 282), ('community', 282), ('Lady', 282), ('hidden', 282), ('drunk', 282), ('Kevin', 282), ('ill', 281), ('blame', 281), ('nicely', 281), ('fights', 281), ('torture', 281), ('channel', 280), ('anybody', 280), ('constant', 279), ('church', 279), ('Me', 279), ('VHS', 279), ('Superman', 279), ('record', 278), ('cross', 278), ('European', 278), ('desire', 278), ('nonsense', 278), ('Freddy', 278), ('pre', 278), ('Dark', 277), ('Spanish', 277), ('army', 277), ('Nick', 277), ('floor', 276), ('growing', 276), ('hurt', 276), ('fail', 276), ('mixed', 275), ('friendship', 275), ('toward', 275), ('suit', 275), ('animal', 275), ('Tarzan', 275), ('responsible', 275), ('returns', 274), ('treated', 274), ('games', 274), ('Morgan', 274), ('partner', 273), ('pulled', 273), ('humanity', 273), ('villains', 273), ('aware', 273), ('roll', 273), ('noticed', 272), ('met', 272), ('hopes', 272), ('cars', 272), ('results', 272), ('Jimmy', 272), ('Lewis', 272), ('manage', 271), ('quiet', 271), ('realism', 271), ('Dick', 271), ('Simon', 271), ('memories', 270), ('loss', 270), ('included', 270), ('lies', 270), ('understanding', 270), ('hits', 270), ('conflict', 269), ('loose', 269), ('months', 269), ('Gary', 269), ('Lynch', 269), ('twenty', 268), ('bloody', 268), ('sign', 268), ('lacking', 268), ('Show', 268), ('award', 267), ('Washington', 267), ('finest', 267), ('concerned', 267), ('talents', 267), ('skills', 267), ('Mike', 267), ('soldier', 267), ('terribly', 267), ('bland', 267), ('dangerous', 266), ('mine', 266), ('discovered', 266), ('saved', 266), ('author', 266), ('cable', 266), ('Eric', 266), ('perspective', 265), ('delightful', 265), ('Blood', 265), ('rubbish', 265), ('discover', 264), ('Wow', 264), ('finale', 264), ('dealing', 264), ('Gene', 264), ('Anthony', 263), ('cuts', 263), ('unknown', 263), ('dated', 263), ('passion', 263), ('pretentious', 263), ('guns', 263), ('keeping', 263), ('eating', 263), ('Albert', 263), ('portray', 263), ('Santa', 263), ('youth', 262), ('prove', 262), ('Street', 262), ('alien', 262), ('originally', 262), ('THIS', 261), ('regular', 261), ('kinda', 261), ('bigger', 261), ('visit', 261), ('soap', 261), ('gags', 261), ('driving', 261), ('loses', 261), ('locations', 261), ('reviewers', 261), ('behavior', 261), ('humorous', 261), ('psychological', 260), ('machine', 260), ('murdered', 260), ('theatre', 260), ('edited', 260), ('owner', 260), ('opposite', 260), ('continues', 260), ('numerous', 260), ('joy', 260), ('Sean', 260), ('unfunny', 260), ('High', 259), ('Death', 259), ('sucks', 259), ('empty', 259), ('received', 259), ('context', 259), ('horse', 259), ('satire', 258), ('yeah', 258), ('breaks', 258), ('deals', 258), ('bright', 258), ('kick', 258), ('ass', 258), ('existence', 257), ('remembered', 257), ('Mrs', 257), ('traditional', 257), ('captured', 257), ('Last', 257), ('opens', 257), ('Jennifer', 257), ('ups', 257), ('featuring', 257), ('Luke', 257), ('starting', 256), ('advice', 255), ('debut', 255), ('cameo', 255), ('SPOILERS', 255), ('ordinary', 255), ('Bob', 255), ('Howard', 254), ('summer', 254), ('Lord', 254), ('forces', 254), ('wide', 254), ('dimensional', 254), ('efforts', 254), ('ladies', 254), ('revealed', 254), ('faith', 254), ('calls', 253), ('Thomas', 253), ('Harris', 253), ('Avoid', 253), ('Dan', 252), ('learned', 252), ('genuine', 252), ('sing', 252), ('African', 252), ('survive', 251), ('core', 251), ('blue', 251), ('develop', 251), ('current', 251), ('Really', 251), ('witty', 251), ('versions', 251), ('somebody', 251), ('Welles', 251), ('Anna', 250), ('visuals', 250), ('Kong', 250), ('Anne', 250), ('Marie', 250), ('grown', 249), ('decade', 249), ('grew', 249), ('comparison', 249), ('genuinely', 248), ('awkward', 248), ('seven', 248), ('proved', 248), ('gangster', 248), ('Cage', 248), ('reach', 247), ('officer', 247), ('reaction', 247), ('allows', 247), ('shallow', 247), ('formula', 247), ('singer', 247), ('Stone', 247), ('segment', 247), ('types', 246), ('eat', 246), ('radio', 245), ('brilliantly', 245), ('overly', 245), ('creates', 245), ('unexpected', 245), ('study', 245), ('passed', 245), ('leader', 245), ('therefore', 245), ('suffering', 245), ('parody', 245), ('Sinatra', 245), ('site', 244), ('strength', 244), ('steal', 244), ('higher', 244), ('references', 244), ('spectacular', 244), ('program', 244), ('frame', 244), ('Time', 244), ('failure', 244), ('Much', 244), ('trust', 243), ('Part', 243), ('stereotypes', 243), ('favor', 243), ('Barbara', 243), ('screaming', 242), ('delivered', 242), ('Watching', 242), ('unable', 242), ('wedding', 242), ('sucked', 242), ('sake', 242), ('sheer', 242), ('technology', 241), ('connection', 241), ('Robin', 241), ('Anyone', 241), ('executed', 241), ('relief', 241), ('blind', 241), ('powers', 241), ('discovers', 240), ('fate', 240), ('curious', 240), ('lesson', 239), ('Wars', 239), ('morning', 239), ('travel', 239), ('corny', 239), ('window', 239), ('laughter', 239), ('fault', 239), ('Steven', 239), ('board', 238), ('pair', 238), ('Fox', 238), ('Parker', 238), ('Are', 238), ('gonna', 238), ('entertained', 237), ('caused', 237), ('luck', 237), ('majority', 237), ('built', 237), ('Gordon', 237), ('bank', 237), ('Broadway', 237), ('emotionally', 237), ('More', 236), ('protagonist', 236), ('monsters', 236), ('relate', 235), ('flashbacks', 235), ('families', 235), ('clean', 235), ('Daniel', 235), ('spoilers', 235), ('driven', 235), ('vehicle', 234), ('Academy', 234), ('Men', 234), ('commercial', 234), ('attitude', 234), ('gold', 234), ('painfully', 234), ('Wayne', 234), ('treatment', 233), ('levels', 233), ('Sadly', 233), ('ultimate', 233), ('blonde', 233), ('crappy', 233), ('meeting', 232), ('logic', 232), ('identity', 232), ('individual', 232), ('occasionally', 232), ('graphic', 232), ('sell', 232), ('wit', 232), ('France', 232), ('Halloween', 232), ('obsessed', 231), ('Western', 231), ('gory', 231), ('combination', 231), ('gratuitous', 231), ('Joseph', 231), ('ran', 230), ('seat', 230), ('range', 230), ('dressed', 230), ('fill', 230), ('described', 230), ('stock', 230), ('Jackie', 230), ('irritating', 230), ('Victoria', 230), ('magnificent', 229), ('dry', 229), ('aged', 229), ('sympathetic', 229), ('excited', 229), ('Asian', 229), ('bet', 228), ('underrated', 228), ('Again', 228), ('dreadful', 228), ('ring', 228), ('fat', 228), ('Victor', 228), ('decision', 228), ('pleasant', 228), ('IT', 228), ('broken', 228), ('produce', 228), ('believes', 228), ('Prince', 228), ('Cinderella', 228), ('assume', 228), ('tape', 227), ('seasons', 227), ('portrays', 227), ('theaters', 227), ('thank', 227), ('reviewer', 227), ('religion', 227), ('send', 227), ('standing', 227), ('Woody', 227), ('learns', 226), ('asking', 226), ('ages', 226), ('stopped', 226), ('lucky', 226), ('product', 226), ('Alex', 226), ('model', 226), ('La', 226), ('hearted', 225), ('Canadian', 225), ('chosen', 225), ('Being', 225), ('contrived', 225), ('capable', 225), ('Matt', 225), ('Grant', 225), ('None', 225), ('Nancy', 225), ('foot', 224), ('mission', 224), ('Without', 224), ('boat', 224), ('proper', 224), ('essentially', 224), ('practically', 224), ('par', 224), ('ruined', 224), ('endless', 224), ('exploitation', 224), ('involves', 224), ('Bond', 224), ('appealing', 224), ('embarrassing', 224), ('Roy', 224), ('Anderson', 223), ('largely', 223), ('Brown', 223), ('grow', 223), ('utter', 223), ('warm', 223), ('Actually', 223), ('handsome', 223), ('Lost', 223), ('desert', 223), ('Louis', 222), ('normally', 222), ('portraying', 222), ('feet', 222), ('hey', 222), ('May', 222), ('Germany', 222), ('shop', 222), ('insane', 221), ('Patrick', 221), ('cares', 221), ('unrealistic', 221), ('foreign', 221), ('appropriate', 221), ('field', 221), ('choose', 221), ('Later', 221), ('Plus', 221), ('claim', 221), ('streets', 220), ('warning', 220), ('evidence', 220), ('safe', 220), ('Girl', 220), ('whilst', 220), ('Trek', 220), ('contrast', 220), ('Walter', 220), ('costs', 220), ('fame', 219), ('facts', 219), ('Captain', 219), ('dubbed', 219), ('depressing', 219), ('round', 219), ('UK', 219), ('Moore', 219), ('substance', 219), ('zero', 218), ('circumstances', 218), ('saving', 218), ('rescue', 218), ('THAT', 218), ('hearing', 218), ('judge', 218), ('ancient', 218), ('Ryan', 218), ('thoughts', 218), ('vs', 218), ('naive', 217), ('About', 217), ('sequels', 217), ('excitement', 217), ('research', 216), ('crowd', 216), ('visually', 216), ('chick', 216), ('voices', 216), ('Europe', 216), ('village', 216), ('flesh', 216), ('scare', 216), ('cost', 215), ('satisfying', 215), ('whenever', 215), ('Any', 215), ('Never', 215), ('fits', 215), ('tend', 215), ('theatrical', 215), ('talks', 215), ('sports', 215), ('nominated', 214), ('captures', 214), ('marry', 214), ('Park', 214), ('convinced', 214), ('Look', 213), ('generation', 213), ('matters', 213), ('lousy', 213), ('spoil', 213), ('ALL', 213), ('relatively', 213), ('destroy', 213), ('teenagers', 213), ('ball', 213), ('asleep', 213), ('teenager', 213), ('Powell', 213), ('Saturday', 212), ('Wood', 212), ('walks', 212), ('Oliver', 212), ('tedious', 212), ('cash', 212), ('Africa', 212), ('correct', 211), ('st', 211), ('com', 211), ('paper', 211), ('rental', 211), ('covered', 211), ('Apparently', 211), ('amateurish', 211), ('losing', 210), ('fare', 210), ('steals', 210), ('Old', 210), ('strongly', 210), ('depicted', 210), ('disgusting', 210), ('anime', 210), ('bear', 210), ('Murphy', 210), ('insult', 210), ('Hitchcock', 209), ('factor', 209), ('market', 209), ('contemporary', 209), ('handled', 209), ('amateur', 209), ('Evil', 209), ('Che', 209), ('inner', 208), ('offensive', 208), ('unlikely', 208), ('remain', 208), ('clue', 208), ('Besides', 208), ('Russell', 208), ('vote', 207), ('believed', 207), ('category', 207), ('qualities', 207), ('bodies', 207), ('holding', 207), ('Texas', 207), ('Sometimes', 207), ('provided', 206), ('Australian', 206), ('promise', 206), ('rise', 206), ('initial', 206), ('shocked', 206), ('test', 206), ('Kim', 206), ('liners', 206), ('structure', 206), ('bomb', 206), ('costume', 206), ('priest', 206), ('pity', 205), ('Three', 205), ('classics', 205), ('angles', 205), ('sudden', 205), ('touches', 205), ('semi', 205), ('Thank', 205), ('unfortunate', 205), ('viewed', 205), ('bringing', 205), ('display', 205), ('dirty', 205), ('scale', 205), ('horribly', 205), ('drag', 205), ('NO', 205), ('pile', 205), ('virtually', 205), ('football', 205), ('lawyer', 204), ('refreshing', 204), ('claims', 204), ('section', 204), ('Basically', 204), ('mini', 204), ('During', 204), ('Yeah', 204), ('Columbo', 204), ('baseball', 204), ('Page', 204), ('evening', 203), ('lower', 203), ('cutting', 203), ('green', 203), ('promising', 203), ('makeup', 203), ('focused', 203), ('fly', 203), ('witch', 203), ('movement', 202), ('recall', 202), ('source', 202), ('plans', 202), ('Clark', 202), ('training', 202), ('chose', 202), ('reporter', 202), ('accents', 202), ('serves', 202), ('designed', 202), ('uncle', 202), ('latest', 201), ('surprises', 201), ('influence', 201), ('Our', 201), ('mainstream', 201), ('suffers', 201), ('besides', 201), ('ruin', 201), ('Caine', 201), ('speaks', 201), ('Emma', 201), ('Wilson', 200), ('experiences', 200), ('hanging', 200), ('Meanwhile', 200), ('deaths', 200), ('Festival', 200), ('print', 200), ('degree', 200), ('Brothers', 200), ('Roger', 200), ('Rob', 200), ('Movies', 200), ('continuity', 200), ('Pacino', 200), ('presents', 199), ('Edward', 199), ('surreal', 199), ('politics', 199), ('related', 199), ('propaganda', 199), ('festival', 199), ('realizes', 199), ('speech', 199), ('repeated', 199), ('accidentally', 199), ('skip', 199), ('MGM', 199), ('forgettable', 199), ('closer', 198), ('notch', 198), ('supernatural', 198), ('harsh', 198), ('draw', 198), ('previously', 198), ('routine', 198), ('States', 198), ('cartoons', 198), ('drop', 198), ('hide', 198), ('mistakes', 198), ('convince', 197), ('breaking', 197), ('danger', 197), ('faced', 197), ('weeks', 197), ('creatures', 197), ('uninteresting', 197), ('private', 197), ('killers', 197), ('la', 196), ('witness', 196), ('path', 196), ('teens', 196), ('frankly', 196), ('center', 196), ('struggling', 196), ('Especially', 196), ('Come', 196), ('Queen', 196), ('nude', 195), ('appreciated', 195), ('Lugosi', 195), ('skin', 195), ('highlight', 195), ('legend', 194), ('rules', 194), ('sharp', 194), ('Second', 194), ('cant', 194), ('Chan', 194), ('Take', 194), ('Sarah', 194), ('Zombie', 194), ('Alice', 194), ('Pitt', 194), ('Rose', 193), ('committed', 193), ('Baby', 193), ('United', 193), ('recognize', 193), ('freedom', 193), ('frightening', 193), ('VERY', 193), ('walked', 193), ('experienced', 192), ('offered', 192), ('erotic', 192), ('combined', 192), ('Sullivan', 192), ('Killer', 192), ('narration', 191), ('Irish', 191), ('till', 191), ('kinds', 191), ('Sci', 191), ('surface', 191), ('occasional', 191), ('Donald', 191), ('directly', 190), ('Friday', 190), ('according', 190), ('abuse', 190), ('forth', 190), ('gritty', 190), ('sorts', 190), ('target', 190), ('reputation', 190), ('Bourne', 190), ('Drew', 190), ('changing', 189), ('reveal', 189), ('California', 189), ('Boy', 189), ('Hong', 189), ('Island', 189), ('Hell', 189), ('figures', 189), ('Vietnam', 189), ('atrocious', 189), ('colors', 188), ('spends', 188), ('dollars', 188), ('castle', 188), ('trilogy', 188), ('Back', 188), ('performed', 188), ('paint', 188), ('Sunday', 188), ('Rachel', 188), ('dozen', 188), ('execution', 188), ('Get', 188), ('belief', 188), ('featured', 188), ('remotely', 188), ('price', 187), ('suspenseful', 187), ('false', 187), ('regret', 187), ('melodrama', 187), ('speed', 187), ('effectively', 187), ('downright', 187), ('friendly', 187), ('massive', 187), ('grand', 187), ('Green', 187), ('Unlike', 187), ('grace', 187), ('explains', 187), ('required', 187), ('passing', 187), ('Hoffman', 187), ('junk', 187), ('everywhere', 186), ('legendary', 186), ('worthwhile', 186), ('fictional', 186), ('figured', 186), ('account', 186), ('imagery', 186), ('guilty', 186), ('hired', 186), ('stolen', 186), ('anger', 186), ('eight', 186), ('buddy', 186), ('extras', 186), ('placed', 186), ('Freeman', 186), ('jobs', 186), ('Max', 186), ('scares', 186), ('werewolf', 186), ('Ted', 186), ('Otherwise', 185), ('Johnson', 185), ('insight', 185), ('Someone', 185), ('crying', 185), ('haunted', 185), ('Definitely', 185), ('Music', 185), ('river', 185), ('subtitles', 185), ('twisted', 185), ('favorites', 185), ('bothered', 185), ('BBC', 185), ('statement', 185), ('Jon', 185), ('naturally', 184), ('spoiler', 184), ('proud', 184), ('Each', 184), ('wear', 184), ('sympathy', 184), ('Branagh', 184), ('starred', 183), ('buying', 183), ('pulls', 183), ('Hey', 183), ('Have', 183), ('unconvincing', 183), ('trapped', 183), ('awards', 182), ('Lisa', 182), ('reveals', 182), ('Mexico', 182), ('handed', 182), ('amazed', 182), ('focuses', 182), ('multiple', 182), ('blown', 182), ('brave', 182), ('crude', 182), ('Devil', 182), ('Ron', 182), ('stayed', 182), ('blah', 182), ('Larry', 181), ('abandoned', 181), ('lesbian', 181), ('idiot', 181), ('magical', 181), ('views', 181), ('device', 181), ('rough', 181), ('Susan', 181), ('Julia', 181), ('bore', 181), ('Christ', 181), ('tight', 181), ('Sky', 181), ('page', 181), ('Ned', 181), ('scenario', 180), ('status', 180), ('stays', 180), ('Full', 180), ('paying', 180), ('Dean', 180), ('winner', 180), ('veteran', 180), ('significant', 180), ('Moon', 180), ('Things', 180), ('minds', 180), ('San', 180), ('Sidney', 180), ('vampires', 180), ('achieve', 179), ('delivery', 179), ('format', 179), ('necessarily', 179), ('exact', 179), ('Over', 179), ('bus', 179), ('skill', 179), ('variety', 179), ('heavily', 178), ('spite', 178), ('security', 178), ('conversation', 178), ('India', 178), ('weekend', 178), ('position', 178), ('sensitive', 178), ('Titanic', 177), ('YOU', 177), ('Mexican', 177), ('teeth', 177), ('king', 177), ('Police', 177), ('MST', 177), ('understood', 177), ('Nazi', 177), ('suffer', 177), ('breath', 177), ('prior', 176), ('decades', 176), ('deeper', 176), ('flashback', 176), ('destroyed', 176), ('complicated', 176), ('intellectual', 176), ('wind', 176), ('pregnant', 175), ('table', 175), ('murderer', 175), ('listening', 175), ('cultural', 175), ('mature', 175), ('renting', 175), ('Dennis', 175), ('Your', 175), ('gas', 174), ('expert', 174), ('environment', 174), ('Sorry', 174), ('Does', 174), ('Army', 174), ('stereotypical', 174), ('theory', 174), ('inept', 174), ('sounded', 174), ('Brad', 174), ('slapstick', 173), ('Something', 173), ('task', 173), ('Elizabeth', 173), ('racist', 173), ('facial', 173), ('welcome', 173), ('settings', 173), ('blow', 173), ('fu', 173), ('helping', 173), ('closing', 173), ('answers', 173), ('Maria', 173), ('Lincoln', 173), ('Leslie', 172), ('campy', 172), ('Warner', 172), ('forgot', 172), ('Julie', 172), ('description', 172), ('artists', 172), ('haunting', 172), ('screening', 172), ('prefer', 172), ('Fi', 172), ('depiction', 172), ('ludicrous', 172), ('quirky', 171), ('Acting', 171), ('interview', 171), ('disbelief', 171), ('convey', 171), ('extent', 171), ('expression', 171), ('Soon', 171), ('native', 171), ('examples', 171), ('greater', 171), ('suck', 171), ('peace', 171), ('Blue', 171), ('MOVIE', 170), ('choices', 170), ('critical', 170), ('birth', 170), ('criminals', 170), ('encounter', 170), ('desperately', 170), ('Michelle', 170), ('sisters', 170), ('con', 170), ('spoof', 170), ('funnier', 170), ('entertain', 170), ('Seriously', 170), ('Claire', 170), ('comical', 169), ('faithful', 169), ('mere', 169), ('criticism', 169), ('psycho', 169), ('China', 169), ('storytelling', 169), ('reminiscent', 169), ('dress', 169), ('picks', 169), ('mildly', 169), ('Next', 169), ('sea', 169), ('basis', 169), ('Rogers', 169), ('prepared', 168), ('True', 168), ('IN', 168), ('North', 168), ('novels', 168), ('touched', 168), ('titles', 168), ('drinking', 168), ('Stanley', 168), ('department', 168), ('praise', 168), ('raised', 168), ('cell', 168), ('prime', 168), ('crash', 168), ('afternoon', 168), ('heck', 168), ('Sandler', 168), ('throws', 167), ('crafted', 167), ('handle', 167), ('beloved', 167), ('stomach', 167), ('base', 167), ('indie', 167), ('aka', 167), ('originality', 167), ('tiny', 167), ('kung', 167), ('jail', 167), ('Guy', 167), ('treasure', 166), ('Brooks', 166), ('succeeds', 166), ('lights', 166), ('truck', 166), ('causes', 166), ('Such', 166), ('lonely', 166), ('Miller', 166), ('timing', 166), ('provoking', 166), ('term', 166), ('inspiration', 166), ('enemy', 166), ('throwing', 166), ('spoken', 166), ('properly', 166), ('extraordinary', 166), ('Grade', 166), ('Burt', 166), ('Jessica', 166), ('Hardy', 166), ('authentic', 165), ('SPOILER', 165), ('Space', 165), ('protect', 165), ('serve', 165), ('introduction', 165), ('regard', 165), ('caring', 165), ('international', 165), ('via', 165), ('warned', 165), ('musicals', 165), ('breathtaking', 164), ('hat', 164), ('Cooper', 164), ('calling', 164), ('TO', 164), ('reference', 164), ('sword', 164), ('Matrix', 164), ('Tracy', 164), ('established', 163), ('gross', 163), ('carried', 163), ('universe', 163), ('headed', 163), ('summary', 163), ('USA', 163), ('purely', 163), ('weapons', 163), ('carries', 163), ('productions', 163), ('mask', 163), ('Be', 162), ('Out', 162), ('DO', 162), ('challenge', 162), ('workers', 162), ('lie', 162), ('arrives', 162), ('carrying', 162), ('remote', 162), ('replaced', 162), ('confusion', 162), ('interpretation', 162), ('sitcom', 162), ('sleazy', 162), ('gruesome', 162), ('minded', 162), ('happiness', 162), ('quest', 162), ('rule', 162), ('seek', 162), ('Philip', 162), ('rights', 162), ('escapes', 162), ('Holmes', 162), ('sheriff', 162), ('successfully', 161), ('locked', 161), ('embarrassed', 161), ('sold', 161), ('join', 161), ('tradition', 161), ('daughters', 161), ('maker', 161), ('ignore', 161), ('essential', 161), ('Channel', 161), ('enjoying', 161), ('Reed', 161), ('graphics', 161), ('halfway', 161), ('existent', 161), ('lazy', 161), ('hip', 161), ('Return', 161), ('Give', 160), ('flow', 160), ('jumps', 160), ('exists', 160), ('mentally', 160), ('everyday', 160), ('balance', 160), ('presentation', 160), ('attacked', 160), ('cases', 160), ('determined', 160), ('service', 160), ('driver', 160), ('fairy', 160), ('wears', 160), ('sinister', 160), ('Blair', 160), ('Stanwyck', 160), ('obnoxious', 160), ('Sutherland', 160), ('deadly', 159), ('Laura', 159), ('regarding', 159), ('clips', 159), ('bitter', 159), ('rolling', 159), ('wins', 159), ('arms', 159), ('Brando', 159), ('Bettie', 159), ('ratings', 158), ('Probably', 158), ('adventures', 158), ('intensity', 158), ('Along', 158), ('Jewish', 158), ('topic', 158), ('WWII', 158), ('Power', 158), ('charge', 158), ('goofy', 158), ('laid', 158), ('Lucy', 158), ('raise', 158), ('Whatever', 158), ('attempting', 158), ('retarded', 158), ('Obviously', 157), ('Long', 157), ('fabulous', 157), ('sides', 157), ('text', 157), ('sum', 157), ('remind', 156), ('rival', 156), ('horrific', 156), ('risk', 156), ('bound', 156), ('ashamed', 156), ('General', 156), ('personalities', 156), ('Doctor', 156), ('dogs', 156), ('punch', 156), ('mountain', 156), ('Planet', 156), ('Jake', 156), ('Matthau', 156), ('chilling', 155), ('nightmare', 155), ('poignant', 155), ('chief', 155), ('Had', 155), ('dude', 155), ('stops', 155), ('packed', 155), ('thrilling', 154), ('Spielberg', 154), ('learning', 154), ('upset', 154), ('delight', 154), ('expressions', 154), ('revolves', 154), ('refuses', 154), ('Way', 154), ('hole', 154), ('Jay', 154), ('spy', 154), ('park', 154), ('wishes', 154), ('Bo', 154), ('sleeping', 154), ('struggles', 154), ('frequently', 154), ('Nightmare', 154), ('Rochester', 154), ('screenwriter', 154), ('overcome', 153), ('lesser', 153), ('credible', 153), ('tongue', 153), ('angle', 153), ('attacks', 153), ('FBI', 153), ('cheese', 153), ('served', 153), ('Sir', 153), ('advantage', 153), ('ghosts', 153), ('Widmark', 153), ('flawed', 152), ('tense', 152), ('obsession', 152), ('Heaven', 152), ('interviews', 152), ('metal', 152), ('cynical', 152), ('Bette', 152), ('mansion', 152), ('mindless', 152), ('albeit', 152), ('credibility', 152), ('alike', 152), ('Helen', 152), ('Up', 152), ('Italy', 152), ('legs', 151), ('broke', 151), ('scream', 151), ('mass', 151), ('amazingly', 151), ('arm', 151), ('opened', 151), ('Streisand', 151), ('Mystery', 151), ('stupidity', 151), ('Baker', 150), ('suffered', 150), ('Award', 150), ('medical', 150), ('trite', 150), ('Cat', 150), ('Burton', 150), ('dislike', 150), ('Andrews', 150), ('perform', 150), ('wealthy', 150), ('warn', 150), ('shape', 150), ('greatly', 150), ('Family', 150), ('comedian', 150), ('ironic', 150), ('fallen', 150), ('stylish', 150), ('guessing', 150), ('heaven', 150), ('alright', 150), ('drives', 150), ('controversial', 150), ('manager', 150), ('Derek', 150), ('Seagal', 150), ('succeed', 149), ('gripping', 149), ('marvelous', 149), ('patient', 149), ('seeking', 149), ('performers', 149), ('technique', 149), ('Britain', 149), ('chair', 149), ('Bollywood', 149), ('amongst', 149), ('Carter', 149), ('enjoyment', 149), ('Check', 149), ('countries', 149), ('guard', 149), ('glimpse', 149), ('uncomfortable', 149), ('Kill', 149), ('wasting', 149), ('initially', 148), ('Karloff', 148), ('dubbing', 148), ('Highly', 148), ('franchise', 148), ('innocence', 148), ('cruel', 148), ('ensemble', 148), ('irony', 148), ('separate', 148), ('Dog', 148), ('queen', 148), ('Chaplin', 148), ('REALLY', 148), ('un', 147), ('Hill', 147), ('nowadays', 147), ('Andrew', 147), ('aliens', 147), ('connected', 147), ('tree', 147), ('van', 147), ('adding', 147), ('notable', 147), ('express', 147), ('Kurt', 147), ('shines', 147), ('trial', 147), ('Ralph', 147), ('Nelson', 147), ('forest', 147), ('racism', 146), ('sexuality', 146), ('physically', 146), ('EVER', 146), ('exceptional', 146), ('protagonists', 146), ('President', 146), ('pool', 146), ('aired', 146), ('neat', 146), ('em', 146), ('hundreds', 146), ('Elvira', 146), ('Arnold', 146), ('Candy', 146), ('atmospheric', 146), ('tied', 146), ('plastic', 146), ('Flynn', 146), ('month', 145), ('darkness', 145), ('thousands', 145), ('upper', 145), ('hint', 145), ('spin', 145), ('Home', 145), ('suggests', 145), ('nd', 145), ('Carpenter', 145), ('sings', 145), ('scripts', 145), ('Universal', 145), ('courage', 144), ('nose', 144), ('miles', 144), ('Personally', 144), ('meaningful', 144), ('letting', 144), ('sentimental', 144), ('consists', 144), ('hoped', 144), ('raw', 144), ('lessons', 144), ('noted', 144), ('shorts', 144), ('BAD', 144), ('accepted', 144), ('bag', 144), ('develops', 144), ('cameos', 144), ('lovable', 143), ('thrillers', 143), ('Oscars', 143), ('annoyed', 143), ('lying', 143), ('ice', 143), ('trick', 143), ('jungle', 143), ('spots', 143), ('miscast', 143), ('candy', 143), ('contain', 143), ('Foster', 143), ('idiotic', 143), ('grave', 143), ('hiding', 143), ('Rating', 143), ('condition', 143), ('Lucas', 143), ('Vincent', 143), ('assistant', 143), ('robot', 143), ('Woman', 143), ('belongs', 142), ('terrifying', 142), ('searching', 142), ('gain', 142), ('larger', 142), ('Golden', 142), ('shoots', 142), ('wanna', 142), ('steps', 142), ('hundred', 142), ('Unless', 142), ('mirror', 142), ('Stan', 142), ('troubled', 141), ('Pretty', 141), ('neighborhood', 141), ('equal', 141), ('Roberts', 141), ('Indeed', 141), ('mob', 141), ('happily', 141), ('Final', 141), ('chases', 141), ('object', 141), ('guilt', 141), ('nation', 141), ('Glover', 141), ('turkey', 141), ('Elvis', 141), ('eerie', 140), ('entry', 140), ('Catherine', 140), ('colorful', 140), ('underground', 140), ('concerns', 140), ('guts', 140), ('basement', 140), ('drink', 140), ('Grace', 140), ('Dorothy', 140), ('Hall', 140), ('proof', 140), ('tune', 140), ('Curtis', 140), ('demon', 140), ('Boll', 140), ('complaint', 139), ('per', 139), ('contact', 139), ('saves', 139), ('shut', 139), ('destruction', 139), ('achieved', 139), ('horses', 139), ('hooked', 139), ('infamous', 139), ('terror', 139), ('Hanks', 139), ('solve', 139), ('worry', 139), ('segments', 139), ('court', 139), ('endearing', 139), ('dialogs', 139), ('expensive', 139), ('knife', 139), ('Hamlet', 139), ('essence', 138), ('Denzel', 138), ('hang', 138), ('perfection', 138), ('Eastwood', 138), ('Hudson', 138), ('imaginative', 138), ('dragged', 138), ('portrait', 138), ('adapted', 138), ('concert', 138), ('tour', 138), ('sends', 138), ('ripped', 138), ('dollar', 138), ('competent', 138), ('Mother', 138), ('Lloyd', 138), ('beach', 138), ('split', 138), ('Lane', 138), ('Walken', 138), ('dramas', 137), ('torn', 137), ('PG', 137), ('dinner', 137), ('charisma', 137), ('ultra', 137), ('tribute', 137), ('flaw', 137), ('urban', 137), ('hopefully', 137), ('dealt', 137), ('card', 137), ('weight', 137), ('ad', 137), ('multi', 137), ('spell', 137), ('appearing', 137), ('profound', 137), ('appearances', 137), ('Law', 137), ('Jeremy', 137), ('states', 137), ('Age', 137), ('wave', 137), ('Club', 137), ('Castle', 137), ('teach', 136), ('host', 136), ('fashioned', 136), ('rushed', 136), ('Nevertheless', 136), ('millions', 136), ('SO', 136), ('chasing', 136), ('checking', 136), ('daily', 136), ('Eva', 136), ('gag', 136), ('strangely', 136), ('unforgettable', 136), ('Cusack', 136), ('Comedy', 136), ('Heston', 136), ('goal', 135), ('sexually', 135), ('hitting', 135), ('stealing', 135), ('closely', 135), ('thousand', 135), ('reactions', 135), ('techniques', 135), ('quote', 135), ('boredom', 135), ('repeat', 135), ('specific', 135), ('Almost', 135), ('Chuck', 135), ('pointed', 135), ('Canada', 135), ('fitting', 135), ('striking', 135), ('tricks', 135), ('III', 135), ('plague', 135), ('shower', 135), ('Bobby', 134), ('revolution', 134), ('Neil', 134), ('painting', 134), ('National', 134), ('Ice', 134), ('neighbor', 134), ('neck', 134), ('sky', 134), ('oil', 134), ('covers', 134), ('riding', 134), ('knock', 134), ('countless', 134), ('cousin', 134), ('stunts', 134), ('grows', 134), ('fool', 134), ('madness', 134), ('incoherent', 134), ('intentions', 133), ('inspiring', 133), ('charismatic', 133), ('hype', 133), ('fianc', 133), ('attempted', 133), ('tear', 133), ('carefully', 133), ('Hope', 133), ('surrounded', 133), ('Barry', 133), ('Ghost', 133), ('fish', 133), ('beating', 133), ('experiment', 133), ('Burns', 133), ('battles', 133), ('colour', 133), ('Science', 133), ('Ritter', 133), ('virus', 133), ('stated', 132), ('thirty', 132), ('busy', 132), ('relevant', 132), ('surrounding', 132), ('stronger', 132), ('represents', 132), ('slight', 132), ('Lake', 132), ('Think', 132), ('Zero', 132), ('fired', 132), ('easier', 132), ('sons', 132), ('homage', 132), ('intention', 132), ('Wild', 132), ('walls', 132), ('homeless', 131), ('increasingly', 131), ('Excellent', 131), ('exercise', 131), ('Cox', 131), ('importance', 131), ('stretch', 131), ('occurs', 131), ('requires', 131), ('Cinema', 131), ('suspects', 131), ('kicks', 131), ('messages', 131), ('enters', 131), ('dropped', 131), ('fortune', 131), ('unintentionally', 131), ('horrendous', 131), ('fourth', 131), ('pack', 131), ('Ian', 131), ('Cole', 131), ('Code', 131), ('Astaire', 131), ('invisible', 131), ('dancer', 130), ('Dawson', 130), ('inevitable', 130), ('identify', 130), ('stunt', 130), ('Kubrick', 130), ('Miike', 130), ('Father', 130), ('disagree', 130), ('Poor', 130), ('LA', 130), ('believing', 130), ('Korean', 130), ('drags', 130), ('library', 130), ('Carol', 130), ('contract', 130), ('attached', 130), ('loser', 130), ('School', 130), ('Bakshi', 130), ('flight', 129), ('crisis', 129), ('acceptable', 129), ('thrills', 129), ('brilliance', 129), ('spending', 129), ('professor', 129), ('briefly', 129), ('Master', 129), ('Rather', 129), ('projects', 129), ('Sally', 129), ('glass', 129), ('dare', 129), ('videos', 129), ('Davies', 129), ('East', 129), ('elsewhere', 129), ('encounters', 129), ('Reynolds', 129), ('demons', 129), ('scientists', 129), ('performing', 128), ('enter', 128), ('resolution', 128), ('sophisticated', 128), ('associated', 128), ('killings', 128), ('toilet', 128), ('corrupt', 128), ('attraction', 128), ('Carrey', 128), ('network', 128), ('worker', 128), ('Scooby', 128), ('Jamie', 127), ('wreck', 127), ('creation', 127), ('kidnapped', 127), ('Living', 127), ('pleased', 127), ('struck', 127), ('stood', 127), ('Thanks', 127), ('handful', 127), ('medium', 127), ('weapon', 127), ('insulting', 127), ('commit', 127), ('Uncle', 127), ('strip', 127), ('Eye', 127), ('fest', 127), ('bridge', 127), ('Church', 127), ('selling', 126), ('revelation', 126), ('Whether', 126), ('hunting', 126), ('Plot', 126), ('importantly', 126), ('menacing', 126), ('crimes', 126), ('Spike', 126), ('threatening', 126), ('Die', 126), ('Hunter', 126), ('stranger', 126), ('gift', 126), ('pushed', 126), ('tremendous', 126), ('Australia', 126), ('hunt', 126), ('useless', 126), ('St', 126), ('reunion', 126), ('aforementioned', 126), ('Brosnan', 126), ('Navy', 126), ('Kane', 125), ('honor', 125), ('wondered', 125), ('talked', 125), ('continued', 125), ('flawless', 125), ('nonetheless', 125), ('realise', 125), ('instantly', 125), ('technically', 125), ('Joey', 125), ('listed', 125), ('allowing', 125), ('wake', 125), ('raped', 125), ('Jesse', 125), ('nuclear', 125), ('evident', 125), ('drivel', 125), ('Aside', 125), ('worthless', 124), ('admire', 124), ('photographed', 124), ('sun', 124), ('disease', 124), ('corner', 124), ('beings', 124), ('Films', 124), ('beaten', 124), ('pleasantly', 124), ('Satan', 124), ('investigation', 124), ('breasts', 124), ('individuals', 124), ('creators', 124), ('Add', 124), ('secretary', 124), ('hire', 124), ('rocks', 124), ('Worst', 124), ('Cameron', 123), ('ticket', 123), ('fifteen', 123), ('overlooked', 123), ('Right', 123), ('fears', 123), ('shadow', 123), ('Mitchell', 123), ('estate', 123), ('bucks', 123), ('letter', 123), ('consequences', 123), ('stiff', 123), ('accomplished', 123), ('forgive', 123), ('buried', 123), ('ego', 123), ('appalling', 123), ('attracted', 123), ('push', 123), ('sacrifice', 123), ('generous', 123), ('Cagney', 123), ('CIA', 123), ('vague', 123), ('Catholic', 123), ('planning', 122), ('characterization', 122), ('repetitive', 122), ('string', 122), ('opposed', 122), ('spiritual', 122), ('relative', 122), ('melodramatic', 122), ('prevent', 122), ('Perry', 122), ('comics', 122), ('tons', 122), ('curiosity', 122), ('persona', 122), ('ABC', 122), ('watches', 122), ('Rex', 122), ('vacation', 122), ('Alexander', 122), ('Norman', 122), ('Francisco', 122), ('slightest', 122), ('ambitious', 122), ('wonders', 122), ('holiday', 122), ('Falk', 122), ('nine', 121), ('Gothic', 121), ('size', 121), ('revealing', 121), ('strike', 121), ('Picture', 121), ('health', 121), ('poverty', 121), ('Remember', 121), ('subplot', 121), ('redemption', 121), ('Live', 121), ('spoiled', 121), ('Craig', 121), ('lacked', 121), ('Felix', 121), ('titled', 121), ('Fire', 121), ('Five', 121), ('Emily', 121), ('Dirty', 121), ('Ruth', 120), ('ignored', 120), ('hearts', 120), ('splendid', 120), ('installment', 120), ('slap', 120), ('Lou', 120), ('documentaries', 120), ('directorial', 120), ('national', 120), ('morality', 120), ('competition', 120), ('subsequent', 120), ('motivation', 120), ('jealous', 120), ('Los', 120), ('outrageous', 120), ('conceived', 120), ('strictly', 120), ('remaining', 120), ('shy', 120), ('logical', 120), ('progress', 120), ('critic', 120), ('Goldberg', 120), ('cared', 120), ('factory', 120), ('burn', 119), ('mild', 119), ('whoever', 119), ('oddly', 119), ('Apart', 119), ('pulling', 119), ('partly', 119), ('frustrated', 119), ('smoking', 119), ('distant', 119), ('spooky', 119), ('captivating', 119), ('superbly', 119), ('dig', 119), ('exaggerated', 119), ('butt', 119), ('overdone', 119), ('cabin', 119), ('response', 119), ('Point', 119), ('jerk', 119), ('Ken', 119), ('Murder', 119), ('bullets', 119), ('clues', 119), ('miserably', 119), ('Fay', 119), ('Wes', 119), ('Laurel', 119), ('Mel', 118), ('accused', 118), ('Fisher', 118), ('outcome', 118), ('draws', 118), ('poster', 118), ('achievement', 118), ('lifetime', 118), ('tea', 118), ('explicit', 118), ('Verhoeven', 118), ('NEVER', 118), ('Would', 118), ('obscure', 118), ('eyed', 118), ('dignity', 118), ('Mann', 118), ('Angel', 118), ('reduced', 118), ('noise', 118), ('tales', 118), ('Turner', 118), ('distance', 118), ('futuristic', 118), ('boxing', 118), ('Secret', 118), ('uninspired', 118), ('intrigued', 117), ('elderly', 117), ('toy', 117), ('hence', 117), ('Could', 117), ('reasonable', 117), ('explore', 117), ('Dave', 117), ('Hard', 117), ('darker', 117), ('returning', 117), ('Season', 117), ('connect', 117), ('farce', 117), ('scientific', 117), ('wow', 117), ('Kenneth', 117), ('psychiatrist', 117), ('sticks', 117), ('Dad', 117), ('discussion', 117), ('Ellen', 117), ('elaborate', 117), ('territory', 117), ('reasonably', 117), ('psychotic', 117), ('Indians', 117), ('cardboard', 117), ('Glenn', 117), ('Wells', 117), ('Gandhi', 117), ('repeatedly', 116), ('sole', 116), ('notably', 116), ('jumping', 116), ('throat', 116), ('slave', 116), ('providing', 116), ('imagined', 116), ('rd', 116), ('Inspector', 116), ('exposed', 116), ('scripted', 116), ('notorious', 116), ('Throughout', 116), ('areas', 116), ('aimed', 116), ('shirt', 116), ('returned', 116), ('chain', 116), ('duo', 116), ('internet', 116), ('wannabe', 116), ('pro', 116), ('argument', 116), ('Duke', 116), ('Romero', 116), ('Witch', 116), ('picking', 116), ('magazine', 116), ('Honestly', 116), ('Timothy', 116), ('convoluted', 116), ('minimal', 116), ('rap', 116), ('editor', 115), ('reached', 115), ('complain', 115), ('commercials', 115), ('cried', 115), ('beer', 115), ('parent', 115), ('failing', 115), ('suits', 115), ('rid', 115), ('Real', 115), ('Matthew', 115), ('thick', 115), ('childish', 115), ('sloppy', 115), ('press', 115), ('blockbuster', 115), ('hysterical', 115), ('tortured', 115), ('Except', 115), ('shark', 115), ('Video', 115), ('photographer', 115), ('Grand', 115), ('landscape', 115), ('movements', 115), ('pet', 115), ('load', 115), ('Paulie', 115), ('dire', 115), ('Warren', 114), ('marks', 114), ('Century', 114), ('Princess', 114), ('bond', 114), ('occurred', 114), ('Manhattan', 114), ('arrested', 114), ('Eyre', 114), ('involvement', 114), ('highlights', 114), ('Altman', 114), ('cheek', 114), ('extended', 114), ('arrive', 114), ('kiss', 114), ('choreography', 114), ('Dracula', 114), ('Karen', 114), ('Mildred', 114), ('Cold', 114), ('Kirk', 114), ('unbearable', 114), ('conspiracy', 114), ('narrator', 114), ('lab', 114), ('lake', 114), ('Danes', 114), ('Lily', 114), ('restaurant', 113), ('typically', 113), ('broad', 113), ('improved', 113), ('existed', 113), ('genres', 113), ('blend', 113), ('timeless', 113), ('Ireland', 113), ('afterwards', 113), ('affected', 113), ('worried', 113), ('precious', 113), ('Set', 113), ('differences', 113), ('antics', 113), ('definite', 113), ('ON', 113), ('displays', 113), ('ranks', 113), ('flash', 113), ('Fortunately', 113), ('Wallace', 113), ('threat', 113), ('resembles', 113), ('digital', 113), ('ought', 113), ('intent', 113), ('grim', 113), ('Eventually', 113), ('demands', 113), ('studios', 113), ('dancers', 113), ('population', 113), ('Carrie', 113), ('Hamilton', 113), ('Buck', 113), ('Beatty', 113), ('Modesty', 113), ('receive', 112), ('fancy', 112), ('trade', 112), ('wing', 112), ('sadistic', 112), ('selfish', 112), ('burned', 112), ('enjoys', 112), ('threw', 112), ('bat', 112), ('gotta', 112), ('Soviet', 112), ('border', 112), ('Make', 112), ('idiots', 112), ('gentle', 112), ('cowboy', 112), ('guest', 112), ('occur', 112), ('Ever', 112), ('swear', 112), ('chances', 112), ('recognized', 112), ('Nazis', 112), ('website', 112), ('vicious', 112), ('Pat', 112), ('Miles', 112), ('Brazil', 112), ('argue', 112), ('devil', 112), ('ridiculously', 112), ('absence', 112), ('Summer', 112), ('grandmother', 112), ('symbolism', 112), ('westerns', 112), ('Craven', 112), ('bathroom', 111), ('merit', 111), ('overrated', 111), ('hunter', 111), ('machines', 111), ('screams', 111), ('sadness', 111), ('Hugh', 111), ('burning', 111), ('Spirit', 111), ('Lawrence', 111), ('Down', 111), ('pretend', 111), ('concerning', 111), ('broadcast', 111), ('strikes', 111), ('proceedings', 111), ('Quite', 111), ('Margaret', 111), ('dynamic', 111), ('birthday', 111), ('superficial', 111), ('investigate', 111), ('suited', 111), ('mouse', 111), ('Years', 111), ('pseudo', 111), ('Greek', 111), ('misses', 111), ('Fonda', 111), ('Annie', 111), ('Nicholson', 111), ('Todd', 111), ('horrid', 111), ('Terry', 110), ('heroic', 110), ('Godfather', 110), ('engaged', 110), ('LOVE', 110), ('harder', 110), ('Atlantis', 110), ('Granted', 110), ('End', 110), ('BUT', 110), ('comfortable', 110), ('presumably', 110), ('composed', 110), ('MAN', 110), ('staged', 110), ('Eyes', 110), ('piano', 110), ('styles', 110), ('incident', 110), ('Series', 110), ('explosion', 110), ('glory', 110), ('daring', 110), ('horrors', 110), ('Leonard', 110), ('Lots', 110), ('Rambo', 110), ('Stiller', 110), ('Donna', 109), ('escaped', 109), ('Based', 109), ('thrill', 109), ('contained', 109), ('eccentric', 109), ('popcorn', 109), ('intrigue', 109), ('gradually', 109), ('blows', 109), ('overwhelming', 109), ('aging', 109), ('Art', 109), ('tame', 109), ('Thus', 109), ('thankfully', 109), ('golden', 109), ('Carl', 109), ('dear', 109), ('Kapoor', 109), ('Considering', 109), ('freak', 109), ('ease', 109), ('Fulci', 109), ('Given', 109), ('unpleasant', 109), ('row', 109), ('code', 109), ('clothing', 109), ('journalist', 109), ('Nice', 109), ('Edie', 109), ('Voight', 109), ('Madonna', 109), ('disturbed', 108), ('folk', 108), ('newspaper', 108), ('groups', 108), ('detailed', 108), ('conventional', 108), ('worlds', 108), ('liking', 108), ('loosely', 108), ('coherent', 108), ('Child', 108), ('Worth', 108), ('innovative', 108), ('adequate', 108), ('Clint', 108), ('Kennedy', 108), ('errors', 108), ('conversations', 108), ('abilities', 108), ('devoted', 108), ('Grey', 108), ('Brady', 108), ('noble', 108), ('smoke', 108), ('Walker', 108), ('deliberately', 108), ('Dream', 108), ('nearby', 108), ('Blake', 108), ('Lemmon', 108), ('Palma', 108), ('notes', 107), ('possibility', 107), ('Certainly', 107), ('Rings', 107), ('react', 107), ('unbelievably', 107), ('resemblance', 107), ('craft', 107), ('stone', 107), ('orders', 107), ('philosophy', 107), ('banned', 107), ('murderous', 107), ('HBO', 107), ('cringe', 107), ('rural', 107), ('Cary', 107), ('bare', 107), ('ingredients', 107), ('thumbs', 107), ('offering', 107), ('odds', 107), ('Li', 107), ('prostitute', 107), ('meat', 107), ('explosions', 107), ('brand', 107), ('influenced', 107), ('west', 107), ('cameras', 107), ('builds', 107), ('relies', 107), ('Price', 107), ('kitchen', 107), ('Cube', 107), ('Lion', 107), ('Cuba', 107), ('succeeded', 106), ('Olivier', 106), ('wealth', 106), ('defeat', 106), ('causing', 106), ('distracting', 106), ('producing', 106), ('staying', 106), ('scheme', 106), ('catches', 106), ('According', 106), ('beats', 106), ('beast', 106), ('meaningless', 106), ('blowing', 106), ('satisfied', 106), ('bears', 106), ('occasion', 106), ('unintentional', 106), ('kidding', 106), ('Woods', 106), ('discuss', 106), ('dentist', 106), ('aunt', 106), ('lit', 106), ('implausible', 106), ('unwatchable', 106), ('Imagine', 106), ('foul', 106), ('removed', 106), ('sport', 106), ('Florida', 106), ('passes', 106), ('Rain', 106), ('root', 106), ('Polanski', 106), ('corpse', 106), ('Jeffrey', 105), ('vivid', 105), ('classes', 105), ('drawing', 105), ('Through', 105), ('financial', 105), ('explored', 105), ('trap', 105), ('Seeing', 105), ('Special', 105), ('sentence', 105), ('shine', 105), ('Brooklyn', 105), ('currently', 105), ('league', 105), ('Hank', 105), ('Mad', 105), ('bleak', 105), ('centered', 105), ('juvenile', 105), ('president', 105), ('Seven', 105), ('wicked', 105), ('Dawn', 105), ('careers', 105), ('subjects', 105), ('moon', 105), ('angel', 105), ('souls', 105), ('backdrop', 105), ('mistaken', 105), ('Maggie', 105), ('unfolds', 104), ('impress', 104), ('smaller', 104), ('portion', 104), ('measure', 104), ('whereas', 104), ('afford', 104), ('triumph', 104), ('staff', 104), ('recorded', 104), ('hates', 104), ('Characters', 104), ('Linda', 104), ('uneven', 104), ('primary', 104), ('drunken', 104), ('lifestyle', 104), ('disc', 104), ('Dragon', 104), ('lust', 104), ('Streep', 104), ('notion', 104), ('sidekick', 104), ('Harvey', 104), ('shall', 104), ('Thing', 104), ('offended', 104), ('NOTHING', 104), ('Hurt', 104), ('Holly', 104), ('Cook', 104), ('leg', 103), ('official', 103), ('Neither', 103), ('endings', 103), ('alas', 103), ('represent', 103), ('Fu', 103), ('performer', 103), ('countryside', 103), ('smooth', 103), ('models', 103), ('dialogues', 103), ('web', 103), ('liberal', 103), ('consistently', 103), ('mixture', 103), ('tracks', 103), ('River', 103), ('thugs', 103), ('devoid', 103), ('dozens', 103), ('shadows', 103), ('Mickey', 103), ('decisions', 103), ('Rick', 103), ('shoes', 103), ('doors', 103), ('explaining', 103), ('Swedish', 103), ('frustration', 103), ('snow', 103), ('Hills', 103), ('clumsy', 103), ('Ginger', 103), ('superhero', 103), ('shelf', 103), ('fetched', 102), ('directs', 102), ('benefit', 102), ('reaches', 102), ('crack', 102), ('hatred', 102), ('enemies', 102), ('Line', 102), ('goodness', 102), ('avoided', 102), ('fond', 102), ('exotic', 102), ('storm', 102), ('duty', 102), ('trees', 102), ('describes', 102), ('cheating', 102), ('Tommy', 102), ('drops', 102), ('celluloid', 102), ('travels', 102), ('AT', 102), ('politically', 102), ('Garbo', 102), ('audio', 102), ('Legend', 102), ('coffee', 102), ('props', 102), ('buddies', 102), ('houses', 102), ('trio', 102), ('blatant', 102), ('Scream', 102), ('Roman', 102), ('curse', 102), ('Amy', 102), ('Kid', 102), ('Diane', 102), ('robots', 102), ('dating', 102), ('flop', 102), ('synopsis', 101), ('backgrounds', 101), ('desperation', 101), ('pays', 101), ('hints', 101), ('ruthless', 101), ('Top', 101), ('Diana', 101), ('spirited', 101), ('primarily', 101), ('ha', 101), ('doomed', 101), ('buff', 101), ('International', 101), ('painted', 101), ('montage', 101), ('highest', 101), ('lyrics', 101), ('outfit', 101), ('combat', 101), ('Vampire', 101), ('enormous', 101), ('urge', 101), ('Macy', 101), ('Bottom', 101), ('possessed', 101), ('authority', 101), ('Shirley', 101), ('Angeles', 101), ('brains', 101), ('immensely', 101), ('captain', 101), ('Hartley', 101), ('Skip', 101), ('hideous', 101), ('stinker', 101), ('motives', 100), ('secrets', 100), ('generic', 100), ('April', 100), ('planned', 100), ('developing', 100), ('survival', 100), ('companion', 100), ('claimed', 100), ('FX', 100), ('matches', 100), ('relations', 100), ('emphasis', 100), ('remarkably', 100), ('forms', 100), ('commented', 100), ('bedroom', 100), ('vast', 100), ('Kids', 100), ('Super', 100), ('nevertheless', 100), ('Homer', 100), ('advance', 100), ('signs', 100), ('Hood', 100), ('disappear', 100), ('Niro', 100), ('Griffith', 100), ('block', 100), ('senseless', 100), ('wont', 100), ('damage', 100), ('Doo', 100), ('Lumet', 100), ('unsettling', 99), ('displayed', 99), ('tie', 99), ('awe', 99), ('Jonathan', 99), ('ONE', 99), ('dedicated', 99), ('disappeared', 99), ('transformation', 99), ('officers', 99), ('discovery', 99), ('interaction', 99), ('endure', 99), ('Beautiful', 99), ('Shaw', 99), ('involve', 99), ('disjointed', 99), ('stellar', 99), ('weakest', 99), ('adorable', 99), ('focusing', 99), ('Godzilla', 99), ('rank', 99), ('guessed', 99), ('introduces', 99), ('lasted', 99), ('von', 99), ('Times', 99), ('Berlin', 99), ('simplistic', 99), ('executive', 99), ('topless', 99), ('Judy', 99), ('Winters', 99), ('Uwe', 99), ('justify', 98), ('user', 98), ('rooms', 98), ('traveling', 98), ('severe', 98), ('miserable', 98), ('Note', 98), ('warming', 98), ('winds', 98), ('Hal', 98), ('terrorist', 98), ('philosophical', 98), ('practice', 98), ('Fear', 98), ('represented', 98), ('formulaic', 98), ('grab', 98), ('disappoint', 98), ('Saw', 98), ('earned', 98), ('mate', 98), ('aid', 98), ('farm', 98), ('possibilities', 98), ('honesty', 98), ('mountains', 98), ('silver', 98), ('surfing', 98), ('funeral', 98), ('stereotype', 98), ('bullet', 98), ('Hart', 98), ('illogical', 98), ('chased', 98), ('Charlotte', 98), ('Save', 98), ('Wendy', 98), ('Akshay', 98), ('Morris', 97), ('Sandra', 97), ('faster', 97), ('thief', 97), ('faults', 97), ('arrogant', 97), ('Actor', 97), ('similarities', 97), ('settle', 97), ('nervous', 97), ('monkey', 97), ('Year', 97), ('altogether', 97), ('Somehow', 97), ('Friends', 97), ('improve', 97), ('pitch', 97), ('Amanda', 97), ('ideal', 97), ('flies', 97), ('promised', 97), ('loyal', 97), ('thru', 97), ('grasp', 97), ('Scary', 97), ('instant', 97), ('isolated', 97), ('sounding', 97), ('ANY', 97), ('ears', 97), ('toys', 97), ('solely', 97), ('cards', 97), ('Nobody', 97), ('Adams', 97), ('Brother', 97), ('Campbell', 97), ('seventies', 97), ('trashy', 97), ('Days', 97), ('kicked', 97), ('Franco', 97), ('silence', 96), ('forty', 96), ('Leo', 96), ('Women', 96), ('corruption', 96), ('cliche', 96), ('thoughtful', 96), ('subtlety', 96), ('inventive', 96), ('concern', 96), ('link', 96), ('bold', 96), ('consistent', 96), ('dinosaurs', 96), ('tad', 96), ('agrees', 96), ('blew', 96), ('passionate', 96), ('widow', 96), ('considerable', 96), ('Orson', 96), ('racial', 96), ('ignorant', 96), ('choreographed', 96), ('Cut', 96), ('cats', 96), ('depressed', 96), ('pride', 96), ('Christy', 96), ('Grinch', 96), ('Gadget', 96), ('nurse', 95), ('riveting', 95), ('constructed', 95), ('stress', 95), ('plant', 95), ('wished', 95), ('treats', 95), ('buffs', 95), ('nostalgic', 95), ('loads', 95), ('scope', 95), ('translation', 95), ('tunes', 95), ('bone', 95), ('understandable', 95), ('sits', 95), ('photos', 95), ('glorious', 95), ('defend', 95), ('nomination', 95), ('Four', 95), ('Simpson', 95), ('Roth', 95), ('Gore', 95), ('finger', 95), ('hurts', 95), ('mill', 95), ('Bergman', 95), ('Sellers', 95), ('inane', 95), ('Midnight', 95), ('Montana', 95), ('switch', 95), ('Dalton', 95), ('embarrassment', 95), ('Ollie', 95), ('tender', 94), ('useful', 94), ('preview', 94), ('Wonderful', 94), ('corporate', 94), ('poetic', 94), ('contrary', 94), ('dances', 94), ('delivering', 94), ('Bela', 94), ('cup', 94), ('purchase', 94), ('wore', 94), ('Khan', 94), ('wet', 94), ('hardcore', 94), ('Mario', 94), ('taught', 94), ('swimming', 94), ('humble', 94), ('Needless', 94), ('undoubtedly', 94), ('pretending', 94), ('waited', 94), ('hill', 94), ('Paltrow', 94), ('eighties', 94), ('shortly', 94), ('revolutionary', 94), ('Carradine', 94), ('rat', 94), ('Connery', 94), ('agents', 94), ('Run', 94), ('cinematographer', 94), ('Shelley', 94), ('downhill', 94), ('Usually', 94), ('Stevens', 94), ('Myers', 94), ('Dixon', 94), ('Lena', 94), ('Timon', 94), ('valuable', 93), ('affect', 93), ('reads', 93), ('Recommended', 93), ('stole', 93), ('arrived', 93), ('engage', 93), ('Heart', 93), ('inferior', 93), ('suitable', 93), ('trained', 93), ('alcoholic', 93), ('closest', 93), ('confidence', 93), ('Spoilers', 93), ('hook', 93), ('Virginia', 93), ('wound', 93), ('bang', 93), ('Bush', 93), ('Doc', 93), ('Sirk', 93), ('WHY', 93), ('GREAT', 93), ('clown', 93), ('motivations', 93), ('Book', 93), ('convincingly', 93), ('soccer', 93), ('Kyle', 93), ('maintain', 93), ('relation', 93), ('fooled', 93), ('Iran', 93), ('climactic', 93), ('Scottish', 93), ('tiresome', 93), ('Scrooge', 93), ('destroying', 93), ('voiced', 93), ('abysmal', 93), ('Amitabh', 93), ('SNL', 93), ('Hopper', 93), ('crush', 92), ('pot', 92), ('staring', 92), ('boot', 92), ('Edgar', 92), ('horrifying', 92), ('writes', 92), ('behave', 92), ('fatal', 92), ('Early', 92), ('nightmares', 92), ('transfer', 92), ('dub', 92), ('shake', 92), ('sappy', 92), ('gangsters', 92), ('Le', 92), ('beliefs', 92), ('yesterday', 92), ('education', 92), ('laws', 92), ('conflicts', 92), ('chest', 92), ('unhappy', 92), ('Pie', 92), ('jumped', 92), ('spirits', 92), ('secretly', 92), ('closed', 92), ('Nicholas', 92), ('lowest', 92), ('helicopter', 92), ('porno', 92), ('Jenny', 92), ('jazz', 92), ('Gundam', 92), ('coach', 92), ('awake', 92), ('Spock', 92), ('Richardson', 92), ('MacArthur', 92), ('buildings', 91), ('reaching', 91), ('guide', 91), ('desired', 91), ('guarantee', 91), ('mysteries', 91), ('abusive', 91), ('exploration', 91), ('survived', 91), ('transition', 91), ('REAL', 91), ('reflect', 91), ('plight', 91), ('catching', 91), ('Dickens', 91), ('vulnerable', 91), ('spare', 91), ('progresses', 91), ('Stallone', 91), ('remarks', 91), ('tall', 91), ('Dutch', 91), ('catchy', 91), ('safety', 91), ('beneath', 91), ('Raymond', 91), ('scores', 91), ('Germans', 91), ('designs', 91), ('couples', 91), ('celebrity', 91), ('Austen', 91), ('questionable', 91), ('Robinson', 91), ('slick', 91), ('Gerard', 91), ('winter', 91), ('bin', 91), ('Everybody', 91), ('Richards', 91), ('button', 91), ('divorce', 90), ('Writer', 90), ('Gabriel', 90), ('depicts', 90), ('wrapped', 90), ('Absolutely', 90), ('french', 90), ('spoke', 90), ('purchased', 90), ('messed', 90), ('access', 90), ('blond', 90), ('chaos', 90), ('Force', 90), ('homosexual', 90), ('quotes', 90), ('recognition', 90), ('granted', 90), ('Willis', 90), ('spread', 90), ('blank', 90), ('principal', 90), ('Several', 90), ('sh', 90), ('amounts', 90), ('trailers', 90), ('advise', 90), ('Chicago', 90), ('lengthy', 90), ('simplicity', 90), ('construction', 90), ('poetry', 90), ('pants', 90), ('Francis', 90), ('Forget', 90), ('Action', 90), ('specifically', 90), ('punk', 90), ('Vegas', 90), ('chess', 90), ('Chase', 90), ('Cowboy', 90), ('Frankie', 90), ('Boys', 90), ('Domino', 90), ('Frankenstein', 89), ('understated', 89), ('fix', 89), ('bonus', 89), ('relatives', 89), ('Hyde', 89), ('realizing', 89), ('elegant', 89), ('facing', 89), ('lord', 89), ('terrorists', 89), ('vaguely', 89), ('Kay', 89), ('nostalgia', 89), ('Monster', 89), ('Made', 89), ('June', 89), ('CD', 89), ('illegal', 89), ('cave', 89), ('bands', 89), ('stilted', 89), ('joined', 89), ('kicking', 89), ('complexity', 89), ('limits', 89), ('closet', 89), ('Louise', 89), ('potentially', 89), ('cheated', 89), ('prisoners', 89), ('brutally', 89), ('ONLY', 89), ('fighter', 89), ('conservative', 89), ('September', 89), ('artificial', 89), ('advertising', 89), ('copies', 89), ('grandfather', 89), ('Therefore', 89), ('experiments', 89), ('Lundgren', 89), ('heat', 89), ('Alien', 89), ('rendition', 89), ('Beyond', 89), ('Damon', 89), ('resources', 89), ('Farrell', 89), ('Civil', 89), ('Grayson', 89), ('Biko', 89), ('Citizen', 88), ('Dance', 88), ('resort', 88), ('worn', 88), ('matched', 88), ('responsibility', 88), ('challenging', 88), ('rage', 88), ('Ah', 88), ('visits', 88), ('alongside', 88), ('proceeds', 88), ('twin', 88), ('saga', 88), ('fascinated', 88), ('dinosaur', 88), ('viewings', 88), ('patients', 88), ('DON', 88), ('Carell', 88), ('wisdom', 88), ('cake', 88), ('persons', 88), ('attitudes', 88), ('witnessed', 88), ('incomprehensible', 88), ('cure', 88), ('restored', 88), ('Ethan', 88), ('intimate', 88), ('losers', 88), ('acid', 88), ('ridden', 88), ('Funny', 88), ('visible', 88), ('Drake', 88), ('doll', 88), ('Bacall', 88), ('Latin', 88), ('letters', 88), ('Eve', 88), ('Barrymore', 88), ('inappropriate', 88), ('lay', 87), ('opinions', 87), ('Simply', 87), ('ensues', 87), ('yelling', 87), ('Either', 87), ('advanced', 87), ('equivalent', 87), ('Christians', 87), ('widely', 87), ('Face', 87), ('subplots', 87), ('satisfy', 87), ('appreciation', 87), ('eager', 87), ('Theater', 87), ('newly', 87), ('dreary', 87), ('compelled', 87), ('intentionally', 87), ('CG', 87), ('Alone', 87), ('Alison', 87), ('Secondly', 87), ('survivors', 86), ('seeks', 86), ('namely', 86), ('wrap', 86), ('Enjoy', 86), ('demented', 86), ('Wind', 86), ('channels', 86), ('centers', 86), ('tends', 86), ('floating', 86), ('arrival', 86), ('pushing', 86), ('capturing', 86), ('snake', 86), ('et', 86), ('troops', 86), ('Amazing', 86), ('defined', 86), ('ties', 86), ('jaw', 86), ('armed', 86), ('bottle', 86), ('Walsh', 86), ('parties', 86), ('illness', 86), ('Hunt', 86), ('Justin', 86), ('showcase', 86), ('Julian', 86), ('rain', 86), ('bath', 86), ('Zombies', 86), ('huh', 86), ('Novak', 86), ('Belushi', 86), ('GOOD', 86), ('Ashley', 86), ('mundane', 86), ('eaten', 86), ('plausible', 86), ('manipulative', 86), ('wandering', 86), ('Girls', 86), ('robbery', 86), ('Marion', 86), ('Reid', 86), ('Iraq', 85), ('exceptionally', 85), ('Sunshine', 85), ('despair', 85), ('poem', 85), ('capital', 85), ('dropping', 85), ('Furthermore', 85), ('sincere', 85), ('warmth', 85), ('vengeance', 85), ('alternate', 85), ('cleverly', 85), ('Rukh', 85), ('serving', 85), ('Keith', 85), ('History', 85), ('bird', 85), ('belong', 85), ('Better', 85), ('Mars', 85), ('polished', 85), ('Zone', 85), ('Camp', 85), ('Earl', 85), ('bargain', 85), ('witnesses', 85), ('Willie', 85), ('Angels', 85), ('Watson', 85), ('Third', 85), ('mechanical', 85), ('Rooney', 85), ('classical', 85), ('masterpieces', 85), ('Thompson', 85), ('Ruby', 85), ('frequent', 85), ('chooses', 85), ('Alfred', 85), ('methods', 85), ('irrelevant', 85), ('NYC', 85), ('opportunities', 85), ('wrestling', 85), ('Angela', 85), ('Cassidy', 85), ('civilization', 85), ('Cruise', 85), ('Carla', 85), ('Zizek', 85), ('Antwone', 85), ('Molly', 84), ('rush', 84), ('online', 84), ('sink', 84), ('excessive', 84), ('comparing', 84), ('rotten', 84), ('purposes', 84), ('performs', 84), ('MY', 84), ('empathy', 84), ('travesty', 84), ('distinct', 84), ('rubber', 84), ('masterful', 84), ('literature', 84), ('cities', 84), ('creativity', 84), ('species', 84), ('command', 84), ('combine', 84), ('agreed', 84), ('gifted', 84), ('arguably', 84), ('caliber', 84), ('equipment', 84), ('method', 84), ('minimum', 84), ('hilariously', 84), ('suspicious', 84), ('Happy', 84), ('bumbling', 84), ('resist', 84), ('recording', 84), ('creator', 84), ('WW', 84), ('generated', 84), ('Owen', 84), ('Scotland', 84), ('dressing', 84), ('awfully', 84), ('Von', 84), ('mentions', 84), ('icon', 84), ('analysis', 84), ('nephew', 84), ('incompetent', 84), ('Abraham', 84), ('Israel', 84), ('paranoia', 83), ('Naturally', 83), ('lately', 83), ('randomly', 83), ('ruins', 83), ('crucial', 83), ('gender', 83), ('unaware', 83), ('inducing', 83), ('smiling', 83), ('Directed', 83), ('versus', 83), ('warrior', 83), ('waitress', 83), ('lips', 83), ('mile', 83), ('Graham', 83), ('stretched', 83), ('introduce', 83), ('ear', 83), ('popularity', 83), ('birds', 83), ('absent', 83), ('Simmons', 83), ('Meryl', 83), ('phony', 83), ('misery', 83), ('counter', 83), ('signed', 83), ('Kurosawa', 83), ('wishing', 83)]\n"
     ]
    }
   ],
   "source": [
    "top_unigrams = get_top_unigrams(X_train,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LcJpZz6inlbt"
   },
   "outputs": [],
   "source": [
    "def matrice_coocurrence_document_mot(X, top_unigrams):\n",
    "    \"\"\"\n",
    "    Cr√©√© la matrice de co-occurrence document x mot\n",
    "    :param X: list(list(str)), corpus √† preprocess\n",
    "    :param top_unigrams: list(str), top unigrams de train\n",
    "    :return: list(list(int)), la matrice de co-occurrences\n",
    "    \"\"\"\n",
    "    dico_top_unigrams = {}\n",
    "    \n",
    "    matrix_cooccurrence = np.zeros((len(X), len(top_unigrams)))\n",
    "    for i in range(len(top_unigrams)):\n",
    "        dico_top_unigrams[top_unigrams[i]] = i\n",
    "    for k in range(len(X)):\n",
    "        for i in range(len(X[k])):\n",
    "            try:\n",
    "                matrix_cooccurrence[k][dico_top_unigrams[X[k][i]]]+=1\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "    return matrix_cooccurrence\n",
    "\n",
    "matrice_coocurrence_document_mot_train = matrice_coocurrence_document_mot(X_train, top_unigrams)\n",
    "matrice_coocurrence_document_mot_test = matrice_coocurrence_document_mot(X_test, top_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xV7tvzo-ZPVO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [01:54<00:00, 218.31it/s]\n",
      "C:\\Users\\Mehdi EL AYACHI\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [01:56<00:00, 214.37it/s]\n"
     ]
    }
   ],
   "source": [
    "def calculate_TFIDF(X):\n",
    "    \"\"\"\n",
    "    Pond√©ration de la matrice de co-occurrence documents x mots\n",
    "    :param X: list(list(int)), matrice de co-occurrence √† pond√©rer\n",
    "    :return: list(list(int)), matrice de co-occurrence pond√©r√© selon la m√©thode TF-IDF\n",
    "    \"\"\"\n",
    "    weighted_X = np.zeros_like(X)\n",
    "    sum_row = np.sum(X, axis=1)\n",
    "    nb_documents = len(X)\n",
    "    d = np.count_nonzero(X, axis=0)\n",
    "    idf = np.log(nb_documents/d)\n",
    "    for i in trange(len(X)):\n",
    "        for j in range(len(X[0])):\n",
    "            if d[j]>0:\n",
    "                weighted_X[i,j] = (X[i,j]/sum_row[i])*idf[j]\n",
    "    return weighted_X\n",
    "\n",
    "weighted_matrice_coocurrence_document_mot_train = calculate_TFIDF(matrice_coocurrence_document_mot_train)\n",
    "weighted_matrice_coocurrence_document_mot_test = calculate_TFIDF(matrice_coocurrence_document_mot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eu-gIxbkAnKt"
   },
   "source": [
    "### 2.2 Matrice mot √ó mot et PPMI (*positive pointwise mutual information*)\n",
    "\n",
    "Vous devez calculer la m√©trique PPMI. Pour une matrice $m \\times n$ $X$ :\n",
    "\n",
    "\n",
    "$$\\textbf{colsum}(X, j) = \\sum_{i=1}^{m}X_{ij}$$\n",
    "\n",
    "$$\\textbf{sum}(X) = \\sum_{i=1}^{m}\\sum_{j=1}^{n} X_{ij}$$\n",
    "\n",
    "$$\\textbf{expected}(X, i, j) = \n",
    "\\frac{\n",
    "  \\textbf{rowsum}(X, i) \\cdot \\textbf{colsum}(X, j)\n",
    "}{\n",
    "  \\textbf{sum}(X)\n",
    "}$$\n",
    "\n",
    "\n",
    "$$\\textbf{pmi}(X, i, j) = \\log\\left(\\frac{X_{ij}}{\\textbf{expected}(X, i, j)}\\right)$$\n",
    "\n",
    "$$\\textbf{ppmi}(X, i, j) = \n",
    "\\begin{cases}\n",
    "\\textbf{pmi}(X, i, j) & \\textrm{if } \\textbf{pmi}(X, i, j) > 0 \\\\\n",
    "0 & \\textrm{otherwise}\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EXR_zFkuAnKu"
   },
   "source": [
    "**a)**\tA partir des textes du corpus d‚Äôentrainement (neg *et* pos), vous devez construire une matrice de co-occurrence mot √ó mot $M(w,w)$ qui contient les 5000 unigrammes les plus fr√©quents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovIWcX56PAGD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:52<00:00, 478.51it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:48<00:00, 515.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def create_matrice_cooccurence_mot_mot(corpus, top_unigrams):\n",
    "    dico_top_unigrams = {}\n",
    "    cooccurrence_matrix = np.zeros((len(top_unigrams), len(top_unigrams)))\n",
    "    for i in range(len(top_unigrams)):\n",
    "        dico_top_unigrams[top_unigrams[i]] = i\n",
    "        \n",
    "    for sentence in tqdm(corpus):\n",
    "        for i in range(len(sentence)):\n",
    "            try:\n",
    "                index_current_word = dico_top_unigrams[sentence[i]]\n",
    "                for word in np.concatenate((sentence[max(0,i-5):i],sentence[i+1:i+6])):\n",
    "                    try:\n",
    "                        cooccurrence_matrix[index_current_word][dico_top_unigrams[word]]+=1\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "    return cooccurrence_matrix\n",
    "matrice_coocurrence_mot_mot_train = create_matrice_cooccurence_mot_mot(X_train, top_unigrams)\n",
    "matrice_coocurrence_mot_mot_test = create_matrice_cooccurence_mot_mot(X_test, top_unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-tfsnMCAnKy"
   },
   "source": [
    "**b)**\tVous devez cr√©er une fonction `calculate_PPMI` qui prend la matrice $M(w,w)$ et la transforme en une matrice $M‚Äô(w,w)$ avec les valeurs PPMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGD3gm6WAnKz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:31<00:00, 159.86it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:31<00:00, 159.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def calculate_PPMI(X):\n",
    "    \"\"\"\n",
    "    Pond√©ration de la matrice de co-occurrence mots x mots\n",
    "    :param X: list(list(int)), matrice de co-occurrence √† pond√©rer\n",
    "    :return: list(list(int)), matrice de co-occurrence pond√©r√© selon la m√©thode PPMI\n",
    "    \"\"\"\n",
    "    weighted_X = np.zeros_like(X)\n",
    "    sum_col = np.sum(X, axis=0)\n",
    "    sum_row = np.sum(X, axis=1)\n",
    "    sum_X = np.sum(X)\n",
    "    weights = np.transpose(np.dot(np.transpose([sum_col]),[sum_row]))/sum_X\n",
    "    for i in trange(len(X)):\n",
    "        for j in range(len(X[0])):\n",
    "            if weights[i,j]!=0 and X[i,j]>weights[i,j]:\n",
    "                weighted_X[i,j] = np.log(X[i,j]/weights[i,j])\n",
    "    return weighted_X\n",
    "\n",
    "weighted_matrice_coocurrence_mot_mot_train = calculate_PPMI(matrice_coocurrence_mot_mot_train)\n",
    "weighted_matrice_coocurrence_mot_mot_test = calculate_PPMI(matrice_coocurrence_mot_mot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eIRyC-k5AnK2"
   },
   "source": [
    "## 3. Mesures de similarit√©\n",
    "\n",
    "En utilisant le module [scipy.spatial.distance](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html),  d√©finissez des fonctions pour calculer les m√©triques suivantes :\n",
    "\n",
    "**Distance Euclidienne**\n",
    "\n",
    "La distance euclidienne entre deux vecteurs $u$ et $v$ de dimension $n$ est\n",
    "\n",
    "$$\\textbf{euclidean}(u, v) = \n",
    "\\sqrt{\\sum_{i=1}^{n}|u_{i} - v_{i}|^{2}}$$\n",
    "\n",
    "En deux dimensions, cela correspond √† la longueur de la ligne droite entre deux points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nJB3PicsAnK3"
   },
   "source": [
    "**a)** Impl√©mentez la fonction `get_euclidean_distance(v1 ,v2)` qui retourne la distance euclidienne entre les vecteurs v1 et v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ORfX0aJ7AnK4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1 :  [0, 1, 2, 3]\n",
      "v2 :  [1, 2, 3, 4]\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def get_euclidean_distance(v1, v2):\n",
    "    \"\"\"\n",
    "    Cettefonction calcule la distance euclidienne entre deux \n",
    "    vecteurs v1 et v2.\n",
    "    :param v1 : 1-D array, vecteur 1\n",
    "    :param v2 : 1-D array, vecteur 2\n",
    "    :return : float, distance entre les vecteurs v1 et v2\n",
    "    \"\"\"\n",
    "    \n",
    "    return euclidean(v1,v2)\n",
    "\n",
    "#Affichage d'un test :\n",
    "v1, v2 = ([0,1,2,3],[1,2,3,4])\n",
    "print('v1 : ', v1)\n",
    "print('v2 : ', v2)\n",
    "print(get_euclidean_distance(v1,v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lHVOg258AnK8"
   },
   "source": [
    "**Distance Cosinus**\n",
    "\n",
    "\n",
    "La distance cosinus entre deux vecteurs $u$ et $v$ de dimension $n$ s'√©crit :\n",
    "\n",
    "$$\\textbf{cosine}(u, v) = \n",
    "1 - \\frac{\\sum_{i=1}^{n} u_{i} \\cdot v_{i}}{\\|u\\|_{2} \\cdot \\|v\\|_{2}}$$\n",
    "\n",
    "Le terme de droite dans la soustraction mesure l'angle entre $u$ et $v$; on l'appelle la *similarit√© cosinus* entre $u$ et $v$.\n",
    "\n",
    "**b)** Impl√©mentez la fonction `get_cosinus_distance(v1, v2)` qui retourne la distance cosinus entre les vecteurs v1 et v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pcJQI1dhAnK9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1 :  [0, 1, 2, 3]\n",
      "v2 :  [0, 2, 4, 6]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def get_cosinus_distance(v1, v2):\n",
    "    \"\"\"\n",
    "    Cettefonction calcule la distance cosinus entre deux \n",
    "    vecteurs v1 et v2.\n",
    "    :param v1 : 1-D array, vecteur 1\n",
    "    :param v2 : 1-D array, vecteur 2\n",
    "    :return : float, distance entre les vecteurs v1 et v2\n",
    "    \"\"\"\n",
    "    \n",
    "    return cosine(v1,v2)\n",
    "\n",
    "#Affichage d'un test :\n",
    "v1, v2 = ([0,1,2,3],[0,2,4,6])\n",
    "print('v1 : ', v1)\n",
    "print('v2 : ', v2)\n",
    "print(get_cosinus_distance(v1,v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HhGlTOkOAnLB"
   },
   "source": [
    "**c)** Impl√©mentez la fonction `get_most_similar_PPMI(word, metric, n)` qui prend un mot en entr√©e et une mesure de distance et qui retourne les n mots les plus similaires selon la mesure. Les mesures √† tester sont : la distance euclidienne et la distance cosinus implant√©es ci-dessus. Le vecteur du mot word doit √™tre extrait de la matrice $M‚Äô(w,w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5zetRaBVAnLC"
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def get_most_similar_PPMI(word, metric, n):\n",
    "    priority_queue = []\n",
    "    index_word = top_unigrams.index(word)\n",
    "    for i in range(len(weighted_matrice_coocurrence_mot_mot_train)):\n",
    "        if top_unigrams[i] != word:\n",
    "            heapq.heappush(priority_queue, (metric(weighted_matrice_coocurrence_mot_mot_train[index_word],weighted_matrice_coocurrence_mot_mot_train[i]), top_unigrams[i]))\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append(heapq.heappop(priority_queue)[1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UnzUMq5nAnLF"
   },
   "source": [
    "**d)** Trouvez les 5 mots les plus similaires au mot ¬´ bad ¬ª et affichez-les, pour chacune des deux distances. Commentez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZqt7qFQAnLG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 5 mots les plus similaires √† 'bad' avec la distance euclidean :\n",
      "['movie', 'good', 'br', 'really', 'It', 'film', 'This', 'one', 'even', 'The']\n",
      "\n",
      "Les 5 mots les plus similaires √† 'bad' avec la distance cosinus :\n",
      "['awful', 'terrible', 'acting', 'horrible', 'good', 'poor', 'stupid', 'movie', 'script', 'cheesy']\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "print(\"Les 5 mots les plus similaires √† 'bad' avec la distance euclidean :\")\n",
    "print(get_most_similar_PPMI('bad', get_euclidean_distance, n))\n",
    "print()\n",
    "print(\"Les 5 mots les plus similaires √† 'bad' avec la distance cosinus :\")\n",
    "print(get_most_similar_PPMI('bad', get_cosinus_distance, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ry-2_pt3AnLK"
   },
   "source": [
    "-> Commentez ici <-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFy81EDjAnLL"
   },
   "source": [
    "**e)** Impl√©mentez la fonction `get_most_similar_TFIDF(word, metric, n)` qui prend un mot en entr√©e et une mesure de distance et qui retourne les n mots les plus similaires selon la mesure. Les mesures √† tester sont : la distance euclidienne et la distance cosinus implant√©es ci-dessus. Le vecteur du mot word doit √™tre extrait de la matrice $M(d,w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7vRHdDqxAnLM"
   },
   "outputs": [],
   "source": [
    "def get_most_similar_TFIDF(word, metric, n):\n",
    "    priority_queue = []\n",
    "    index_word = top_unigrams.index(word)\n",
    "    for i in range(len(top_unigrams)):\n",
    "        if top_unigrams[i] != word:\n",
    "            heapq.heappush(priority_queue, (metric(weighted_matrice_coocurrence_document_mot_train[:,index_word],weighted_matrice_coocurrence_document_mot_train[:,i]), top_unigrams[i]))\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append(heapq.heappop(priority_queue)[1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awTSwa-7AnLS"
   },
   "source": [
    "**f)** Trouvez les 5 mots les plus similaires au mot ¬´ bad ¬ª et affichez-les, pour chacune des deux distances. Commentez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fKba94sDAnLT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 5 mots les plus similaires √† 'bad' avec la distance euclidean :\n",
      "['The', 'mentions', 'crucial', 'arrival', 'chest', 'aforementioned', 'introduces', 'Secondly', 'Directed', 'namely']\n",
      "\n",
      "Les 5 mots les plus similaires √† 'bad' avec la distance cosinus :\n",
      "['movie', 'acting', 'The', 'good', 'br', 'This', 'like', 'one', 'even', 'worst']\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "print(\"Les 5 mots les plus similaires √† 'bad' avec la distance euclidean :\")\n",
    "print(get_most_similar_TFIDF('bad', get_euclidean_distance, n))\n",
    "print()\n",
    "print(\"Les 5 mots les plus similaires √† 'bad' avec la distance cosinus :\")\n",
    "print(get_most_similar_TFIDF('bad', get_cosinus_distance, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlpvFibXAnLW"
   },
   "source": [
    "*-> Commentez ici <-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ClcQ0MhvAnLX"
   },
   "source": [
    "## 4. Classification de documents avec un mod√®le de langue\n",
    "\n",
    "En vous inspirant de [cet article](https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139), entra√Ænez deux mod√®les de langue $n$-gramme de caract√®re avec lissage de Laplace, l'un sur le corpus `pos`, l'autre sur le corpus `neg`. Puis, pour chaque document $D$, calculez sa probabilit√© selon vos deux mod√®les : $P(D \\mid \\textrm{pos})$ et $P(D \\mid \\textrm{neg})$.\n",
    "\n",
    "Vous pourrez alors pr√©dire sa classe $\\hat{c}_D \\in (\\textrm{pos}, \\textrm{neg})$ en prenant :\n",
    "\n",
    "$$\\hat{c}_D = \\begin{cases}\n",
    "\\textrm{pos} & \\textrm{si } P(D \\mid \\textrm{pos}) > P(D \\mid \\textrm{neg}) \\\\\n",
    "\\textrm{neg} & \\textrm{sinon}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4-y2hGSAnLY",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separation des donn√©es\n",
      "Transformation en minuscule et suppression de la ponctuation pour le set : pos\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a6510447b04c49bd2a3ba08703d9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecte des ngrams pour le \"pos\" set :\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8767f740eb45438cbf9771969b35c147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting du model laplace pour le set : pos\n",
      "Separation des donn√©es\n",
      "Transformation en minuscule et suppression de la ponctuation pour le set : neg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a7db82cb0c46fba6df9c9f69e3558c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecte des ngrams pour le \"neg\" set :\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5caecf59ee482094926a0ef1b02111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting du model laplace pour le set : neg\n"
     ]
    }
   ],
   "source": [
    "#Training models\n",
    "import nltk\n",
    "import string\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#Ordre de notre model\n",
    "order = 4\n",
    "\n",
    "#Creation du vocabulaire \n",
    "vocab = nltk.lm.vocabulary.Vocabulary([chr(i) for i in range(ord('a'), ord('a')+27)]+[' '])\n",
    "\n",
    "#Separation des donn√©es 'pos' et 'neg' :\n",
    "data, n_grams, laplace = {},{},{}\n",
    "for label in ['pos', 'neg']:\n",
    "    print('Separation des donn√©es du set : ',label)\n",
    "    data[label] = np.array(X_train)[np.array(y_train)==label]\n",
    "    #print('Transformation en minuscule et suppression de la ponctuation pour le set : '+label)\n",
    "    #data[label] = [doc.translate(str.maketrans('', '', string.punctuation)).lower() for doc in tqdm(data[label])]\n",
    "    print('Transformation en minuscule et suppression de la ponctuation pour le set : '+label)\n",
    "    data[label] = [\" \".join(doc).translate(str.maketrans('', '', string.punctuation)).lower() for doc in tqdm(data[label])]\n",
    "    print('Collecte des ngrams pour le \"'+label+'\" set :')\n",
    "    n_grams[label] = [ngrams(pad_both_ends(document, order), order) for document in tqdm(data[label])]\n",
    "    laplace[label] = nltk.lm.models.Laplace(order, vocab)\n",
    "    print('Fitting du model laplace pour le set : '+label)\n",
    "    laplace[label].fit(n_grams[label])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f4ArMa_LJm8n"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def laplace_classifier(doc, order = order):\n",
    "    #Cleaning doc from punctuation and tokenizing it\n",
    "    tokens = [ch for ch in \" \".join(doc).translate(str.maketrans('', '', string.punctuation)).lower()]\n",
    "    \n",
    "    #Computing perplexity \n",
    "    perplexity_pos = 0\n",
    "    perplexity_neg = 0\n",
    "    n_grams = list(ngrams(tokens, order, pad_right=True, pad_left=True))\n",
    "    probs = {\"pos\":0,\"neg\":0}\n",
    "    for ngram in n_grams:\n",
    "        for label in probs.keys() :\n",
    "            probs[label] += math.log10(laplace[label].unmasked_score(ngram[-1], ngram[0:-1]))\n",
    "    \n",
    "    classe = [\"neg\",\"pos\"][probs[\"pos\"] > probs[\"neg\"]]\n",
    "    return classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FozNaxLUJpIe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Laplace Classifier :\n",
      "Test  1  :\n",
      "    Predicted classe :  pos \t Target :  pos\n",
      "Test  2  :\n",
      "    Predicted classe :  pos \t Target :  pos\n",
      "Test  3  :\n",
      "    Predicted classe :  pos \t Target :  pos\n",
      "Test  4  :\n",
      "    Predicted classe :  pos \t Target :  pos\n",
      "Test  5  :\n",
      "    Predicted classe :  pos \t Target :  pos\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Laplace Classifier :\")\n",
    "for i in range(5):\n",
    "    print(\"Test \",i+1,\" :\\n    Predicted classe : \" ,laplace_classifier(X_test[i]),\"\\t Target : \",y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEWiIkBLAnLc"
   },
   "source": [
    "## 5. Classification de documents avec sac de mots et Naive Bayes\n",
    "\n",
    "Ici, vous utiliserez l'algorithme Multinomial Naive Bayes (disponible dans [`sklearn.naive_bayes.MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)) pour classifier les documents. Vous utiliserez un mod√®le sac de mots (en anglais *bag of words*, ou BoW) avec TF-IDF pour repr√©senter vos documents.\n",
    "\n",
    "*Note :* vous avez d√©j√† construit la matrice TF-IDF √† la section 2.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0DnTz8nJ3A4"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "26fobjlAAnLd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(weighted_matrice_coocurrence_document_mot_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HS9GHWK6JmJb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Multinomial Naive Bayes Classifier :\n",
      "Test  1  :\n",
      "    Predicted classe :  pos \t Target :  pos\n",
      "Test  2  :\n",
      "    Predicted classe :  pos \t Target :  pos\n",
      "Test  3  :\n",
      "    Predicted classe :  pos \t Target :  pos\n",
      "Test  4  :\n",
      "    Predicted classe :  pos \t Target :  pos\n",
      "Test  5  :\n",
      "    Predicted classe :  pos \t Target :  pos\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Multinomial Naive Bayes Classifier :\")\n",
    "for i in range(5):\n",
    "    print(\"Test \",i+1,\" :\\n    Predicted classe : \" ,\n",
    "          mnb_model.predict([weighted_matrice_coocurrence_document_mot_test[i]])[0],\n",
    "          \"\\t Target : \",y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IGY75wMfAnLg"
   },
   "source": [
    "## 6. Am√©liorations\n",
    "\n",
    "Ici, vous devez proposer une m√©thode d'am√©lioration pour le mod√®le pr√©c√©dent, la justifier et l'impl√©menter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fiXQqg_rAnLh"
   },
   "source": [
    "*-> √âcrivez vos explications ici <-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2RjWD-a_AnLl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur score de precision : 0.8408\n",
      "Meilleur parametre :  {'alpha': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'alpha': [0]+[10**i for i in range(10)]}\n",
    "optimised_mnb = MultinomialNB()\n",
    "gs = GridSearchCV(estimator=optimised_mnb,\n",
    "                     param_grid = parameters,\n",
    "                     scoring='accuracy',\n",
    "                     cv=5,\n",
    "                     n_jobs=-1)\n",
    "\n",
    "# On change la liste des targets pour reduire l'utilisation de m√©moire du GridSearch\n",
    "# Sinon on obtient l'exception MemoryError\n",
    "labels_train = [1 if label=='pos' else 0 for label in y_train]\n",
    "gs.fit(weighted_matrice_coocurrence_document_mot_train, labels_train)\n",
    "print('Meilleur score de precision :', gs.best_score_)\n",
    "print('Meilleur parametre : ', gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Optimised Multinomial Naive Bayes Classifier :\n",
      "Test  1  :\n",
      "    Predicted classe :  pos \t Target :  pos\n",
      "Test  2  :\n",
      "    Predicted classe :  pos \t Target :  pos\n",
      "Test  3  :\n",
      "    Predicted classe :  pos \t Target :  pos\n",
      "Test  4  :\n",
      "    Predicted classe :  pos \t Target :  pos\n",
      "Test  5  :\n",
      "    Predicted classe :  pos \t Target :  pos\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Optimised Multinomial Naive Bayes Classifier :\")\n",
    "for i in range(5):\n",
    "    prediction = 'pos' if gs.predict([weighted_matrice_coocurrence_document_mot_test[i]])[0]==1 else 'neg'\n",
    "    print()\n",
    "    print(\"Test \",i+1,\" :\\n    Predicted classe : \" ,\n",
    "          prediction,\n",
    "          \"\\t Target : \",y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bs1YBBBJAnLo"
   },
   "source": [
    "## 7. √âvaluation\n",
    "\n",
    "√âvaluation des mod√®les des sections 4, 5, 6 sur les donn√©es de test. On attend les m√©triques suivantes : *accuracy*, et pour chaque classe pr√©cision, rappel, score F1. Vous pourrez utiliser le module [`sklearn.metrics`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mVwJyGoBAnLp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test du classifieur  Vanilla Laplace  : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.80      0.86      0.83     12500\n",
      "        pos       0.85      0.78      0.81     12500\n",
      "\n",
      "avg / total       0.82      0.82      0.82     25000\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'columns' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-3e484388499b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test du classifieur \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" : \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"neg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pos\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"neg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pos\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'columns' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import sklearn\n",
    "#predictions = {}\n",
    "#predictions[\"Vanilla Laplace\"] = [laplace_classifier(doc)for doc in tqdm(X_test)]\n",
    "#predictions[\"MNB\"] = mnb_model.predict(weighted_matrice_coocurrence_document_mot_test)\n",
    "#predictions[\"Laplace Am√©lior√©\"] = ['pos' if label==1 else 'neg'\n",
    "#                                   for label in gs.predict(weighted_matrice_coocurrence_document_mot_test)]\n",
    "\n",
    "for model_name in predictions.keys():\n",
    "    print(\"Test du classifieur \",model_name,\" : \")\n",
    "    print(sklearn.metrics.classification_report(y_test, predictions[model_name])) \n",
    "    print(sklearn.metrics.confusion_matrix(y_test, predictions[model_name]), columns=[\"neg\", \"pos\"], index=[\"neg\", \"pos\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CXMk-CPYAnLs"
   },
   "source": [
    "Commentez vos r√©sultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yBPtu16JAnLt"
   },
   "source": [
    "*-> Commentez ici vos r√©sultats <-*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "inf8460_tp2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
