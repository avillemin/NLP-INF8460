{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "DAZzAEuUBb5V",
    "outputId": "798a083b-de80-4a6f-b2a3-a2754cf59130"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JagJC4t2AnJ8"
   },
   "source": [
    "# √âcole Polytechnique de Montr√©al\n",
    "D√©partement G√©nie Informatique et G√©nie Logiciel\n",
    "INF8460 ‚Äì Traitement automatique de la langue naturelle\n",
    "\n",
    "### Prof. Amal Zouaq\n",
    "### Charg√© de laboratoire: F√©lix Martel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuOAuSwsAnJ-"
   },
   "source": [
    "# INF8460 - TP2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pq22DoZgAnJ_"
   },
   "source": [
    "## Objectifs\n",
    "\n",
    "‚Ä¢\tExplorer les mod√®les d‚Äôespaces vectoriels comme repr√©sentations distribu√©es de la s√©mantique des mots et des documents\n",
    "\n",
    "‚Ä¢\tComprendre diff√©rentes mesures de distance entre vecteurs de documents et de mots\n",
    "\n",
    "‚Ä¢\tUtiliser un mod√®le de langue n-gramme de caract√®res et l‚Äôalgorithme Naive Bayes pour l‚Äôanalyse de sentiments dans des revues de films (positives, n√©gatives)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sM-xQOHQAnKA"
   },
   "source": [
    "## 1. Pr√©traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvaRk_TMAnKB"
   },
   "source": [
    "Le jeu de donn√©es est s√©par√© en deux r√©pertoires `train/`et `test`, chacun contenant eux-m√™mes deux sous-r√©pertoires `pos/` et `neg/` pour les revues positives et n√©gatives. Un fichier `readme` d√©crit plus pr√©cis√©ment les donn√©es.\n",
    "\n",
    "Commencez par lire ces donn√©es, en gardant s√©par√©es les donn√©es d'entra√Ænement et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "unsugYpRAnKC",
    "outputId": "cec8d50b-6647-4f6b-dd20-4a540b551d97"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = './data/aclImdb/'\n",
    "\n",
    "def data_reader(path):\n",
    "    \"\"\" Cette fonction lit les donn√©es text et sauvegarde les labels. \n",
    "    \n",
    "    :param path : str, Chemin vers le dossier contenant les donn√©es.\n",
    "    :return X,y: list,list, X est une liste contenant les textes et y est une liste contenant les labels\"\"\"\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    for label in ['pos','neg']:\n",
    "        dir_path = path+label+'/'\n",
    "        for filename in os.listdir(dir_path) :\n",
    "            filepath = dir_path+filename\n",
    "            with open(filepath,'r', encoding = 'utf-8') as file_descriptor:\n",
    "                X.append(file_descriptor.read())\n",
    "                y.append(label)\n",
    "    return X,y\n",
    "\n",
    "X_train, y_train = data_reader(data_path+'train/')\n",
    "X_test, y_test = data_reader(data_path+'test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYEpFxJmAnKF"
   },
   "source": [
    "**a)** Cr√©ez la fonction `clean_doc()` qui effectue les pr√©-traitements suivants‚ÄØ: segmentation en mots‚ÄØ; \n",
    "suppression des signes de ponctuations‚ÄØ; suppression des mots qui contiennent des caract√®res autres qu‚Äôalphab√©tiques‚ÄØ; \n",
    "suppression des mots qui sont connus comme des stop words‚ÄØ; suppression des mots qui ont une longueur de 1 caract√®re. Ensuite, appliquez-la √† vos donn√©es.\n",
    "\n",
    "Les stop words peuvent √™tre obtenus avec `from nltk.corpus import stopwords`. Vous pourrez utiliser des [expressions r√©guli√®res](https://docs.python.org/3.7/howto/regex.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C3K598wKAnKG"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "stop = stopwords.words(\"english\")\n",
    "reTokenizer = RegexpTokenizer('[A-Z|a-z]+')\n",
    "\n",
    "def clean_doc(document):\n",
    "    \"\"\"Effectue les pr√©-traitements suivants‚ÄØ √† un document : \n",
    "    segmentation en mots‚ÄØ; \n",
    "    suppression des signes de ponctuations‚ÄØ; \n",
    "    suppression des mots qui contiennent des caract√®res autres qu‚Äôalphab√©tiques‚ÄØ; \n",
    "    suppression des mots qui sont connus comme des stop words‚ÄØ; \n",
    "    suppression des mots qui ont une longueur de 1 caract√®re.\n",
    "    \n",
    "    :param sentence : str, le texte lu √† partir d'un fichier de donn√©es.\n",
    "    :return : list, liste de tokens trait√©s.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Segmentation en mots et suppression des signes de ponctuation\n",
    "    tokens = reTokenizer.tokenize(document)\n",
    "    #Suppression des stopwords\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    #Suppression des mots d'une longueur de 1 caract√®re\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    return tokens\n",
    "\n",
    "def clean_corpus(corpus):\n",
    "    \"\"\"Effectue les pr√©-traitements suivants‚ÄØ √† l'ensemble du corpus : \n",
    "    segmentation en mots‚ÄØ; \n",
    "    suppression des signes de ponctuations‚ÄØ; \n",
    "    suppression des mots qui contiennent des caract√®res autres qu‚Äôalphab√©tiques‚ÄØ; \n",
    "    suppression des mots qui sont connus comme des stop words‚ÄØ; \n",
    "    suppression des mots qui ont une longueur de 1 caract√®re.\n",
    "    \n",
    "    :param sentence : str, le texte lu √† partir d'un fichier de donn√©es.\n",
    "    :return : list, liste de tokens trait√©s.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [clean_doc(doc) for doc in corpus]\n",
    "    \n",
    "\n",
    "#Affichage d'un test\n",
    "print(X_train[0])\n",
    "print(clean_doc(X_train[0]))\n",
    "\n",
    "#Application de clean_doc √† l'ensemble du corpus\n",
    "X_train = clean_corpus(X_train)\n",
    "X_test = clean_corpus(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Pg5U3cXAnKL"
   },
   "source": [
    "**b)**\tCr√©ez la fonction `build_voc()` qui extrait les unigrammes de l‚Äôensemble d‚Äôentra√Ænement et conserve ceux qui ont une fr√©quence d‚Äôoccurrence de 5 au moins et imprime le nombre de mots dans le vocabulaire. Sauvegardez-le dans un fichier `vocab.txt` (un mot par ligne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnriS8dNAnKM"
   },
   "outputs": [],
   "source": [
    "from nltk.lm.vocabulary import Vocabulary\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "def build_voc(corpus):\n",
    "    \"\"\"\n",
    "    Extrait les unigrammes de l‚Äôensemble d‚Äôentra√Ænement et conserve ceux qui ont une fr√©quence d‚Äôoccurrence de 5 au moins\n",
    "    :param corpus: list(list(str)), un corpus tokeniz√©\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    ngrams, words = padded_everygram_pipeline(1, corpus)\n",
    "    vocab = Vocabulary(words, unk_cutoff=5)\n",
    "    print(\"Nombre de mots dans le vocabulaire:\",len(vocab)-1) \n",
    "    \n",
    "    with open('./output/vocab.txt',\"w\") as f:\n",
    "        for word in list(vocab)[:-1]:\n",
    "            f.write(word+'\\n')\n",
    "    f.close()\n",
    "    return None\n",
    "\n",
    "build_voc(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5yMNwiAAnKQ"
   },
   "source": [
    "**c)** Vous devez cr√©er une fonction `get_top_unigrams(n)` qui retourne les $n$ unigrammes les plus fr√©quents et les affiche, puis l'appeler avec $n=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n9I-NQ8YAnKR"
   },
   "outputs": [],
   "source": [
    "def get_top_unigrams(corpus, n, unk_cutoff = 5):\n",
    "    \"\"\"\n",
    "    Retourne les  ùëõ  unigrammes les plus fr√©quents et les affiche\n",
    "    :param corpus: list(list(str)), un corpus tokeniz√©\n",
    "    :param n: int, nombre d'unigrammes √† afficher\n",
    "    :unk_cutoff: int, le seuil au-dessous duquel un mot est consid√©r√© comme inconnu et remplac√© par <UNK>\n",
    "    :return: list(tuple(str, int)), liste des top unigrams avec leur fr√©quence\n",
    "    \"\"\"\n",
    "    ngrams, words = padded_everygram_pipeline(1, corpus)\n",
    "    vocab = Vocabulary(words, unk_cutoff=unk_cutoff)\n",
    "    most_commun = vocab.counts.most_common(n)\n",
    "    print(most_commun)\n",
    "    return [unigram[0] for unigram in most_commun]\n",
    "\n",
    "most_commun_unigrams = get_top_unigrams(X_train,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1DaK2QfAnKW"
   },
   "source": [
    "**d)**\tVous devez cr√©er une fonction `get_top_unigrams_per_cls(n, cls)` qui retourne les $n$ unigrammes les plus fr√©quents de la classe `cls` (pos ou neg) et les affiche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVycuUqQAnKX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_top_unigrams_per_cls(n, cls, X, y):\n",
    "    \"\"\"\n",
    "    Retourne les  ùëõ  unigrammes les plus fr√©quents de la classe cls (pos ou neg) et les affiche\n",
    "    :param n: int, nombre d'unigrammes √† afficher\n",
    "    :param cls: str, classe √† rechercher (pos ou neg)\n",
    "    :param X: list(list(str)), un corpus tokeniz√©\n",
    "    :param y: list(str), liste des classes\n",
    "    :return: list(str), liste des top unigrams de la bonne classe\n",
    "    \"\"\"\n",
    "    X_cls = list(np.array(X)[np.array(y) == cls])\n",
    "    ngrams, words = padded_everygram_pipeline(1, X_cls)\n",
    "    vocab = Vocabulary(words, unk_cutoff=1)\n",
    "    most_commun = vocab.counts.most_common(n)\n",
    "    print(most_commun)\n",
    "    return [unigram[0] for unigram in most_commun]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dUh-CQQyAnKb"
   },
   "source": [
    "**e)**\tAffichez les 10 unigrammes les plus fr√©quents dans la classe positive :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGuo1CXfAnKc"
   },
   "outputs": [],
   "source": [
    "top_unigrams_per_cls_pos = get_top_unigrams_per_cls(10, \"pos\", X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_k7_n0zAnKg"
   },
   "source": [
    "**f)**\tAffichez les 10 unigrammes les plus fr√©quents dans la classe n√©gative :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RqLKmy8TAnKh"
   },
   "outputs": [],
   "source": [
    "top_unigrams_per_cls_neg = get_top_unigrams_per_cls(10, \"neg\", X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8H1ioGiOAnKk"
   },
   "source": [
    "## 2. Matrices de co-occurence\n",
    "\n",
    "Pour les matrices de cette section, vous pourrez utiliser [des array `numpy`](https://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html) ou des DataFrame [`pandas`](https://pandas.pydata.org/pandas-docs/stable/). \n",
    "\n",
    "Ressources utiles :  le [*quickstart tutorial*](https://numpy.org/devdocs/user/quickstart.html) de numpy et le guide [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FMgl02sqAnKl"
   },
   "source": [
    "### 2.1 Matrice document √ó mot et TF-IDF\n",
    "\n",
    "\n",
    "Soit $X \\in \\mathbb{R}^{m \\times n}$ une matrice de $m$ documents et $n$ mots, telle que $X_{i,j}$ contient la fr√©quence d'occurrence du terme $j$ dans le document $i$ :\n",
    "\n",
    "$$\\textbf{rowsum}(X, d) = \\sum_{j=1}^{n}X_{dj}$$\n",
    "\n",
    "$$\\textbf{TF}(X, d, t) = \\frac{X_{d,t}}{\\textbf{rowsum}(X, d)}$$\n",
    "\n",
    "$$\\textbf{IDF}(X, t) = \\log\\left(\\frac{m}{|\\{d : X_{d,t} > 0\\}|}\\right)$$\n",
    "\n",
    "$$\\textbf{TF-IDF}(X, d, t) = \\textbf{TF}(X, d, t) \\cdot \\textbf{IDF}(X, t)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4a8M98Y8AnKn"
   },
   "source": [
    "En utilisant le m√™me vocabulaire de 5 000 unigrammes, vous devez repr√©senter les documents dans une matrice de co-occurrence document √ó mot $M(d, w)$  et les pond√©rer avec la mesure TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWLNxAFcnm-O"
   },
   "outputs": [],
   "source": [
    "top_unigrams = get_top_unigrams(X_train,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LcJpZz6inlbt"
   },
   "outputs": [],
   "source": [
    "def matrice_coocurrence_document_mot(X, top_unigrams):\n",
    "    \"\"\"\n",
    "    Cr√©√© la matrice de co-occurrence document x mot\n",
    "    :param X: list(list(str)), corpus √† preprocess\n",
    "    :param top_unigrams: list(str), top unigrams de train\n",
    "    :return: list(list(int)), la matrice de co-occurrences\n",
    "    \"\"\"\n",
    "    dico_top_unigrams = {}\n",
    "    \n",
    "    matrix_cooccurrence = np.zeros((len(X), len(top_unigrams)))\n",
    "    for i in range(len(top_unigrams)):\n",
    "        dico_top_unigrams[top_unigrams[i]] = i\n",
    "    for k in range(len(X)):\n",
    "        for i in range(len(X[k])):\n",
    "            try:\n",
    "                matrix_cooccurrence[k][dico_top_unigrams[X[k][i]]]+=1\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "    return matrix_cooccurrence\n",
    "\n",
    "matrice_coocurrence_document_mot_train = matrice_coocurrence_document_mot(X_train, top_unigrams)\n",
    "matrice_coocurrence_document_mot_test = matrice_coocurrence_document_mot(X_test, top_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xV7tvzo-ZPVO"
   },
   "outputs": [],
   "source": [
    "def calculate_TFIDF(X):\n",
    "    \"\"\"\n",
    "    Pond√©ration de la matrice de co-occurrence documents x mots\n",
    "    :param X: list(list(int)), matrice de co-occurrence √† pond√©rer\n",
    "    :return: list(list(int)), matrice de co-occurrence pond√©r√© selon la m√©thode TF-IDF\n",
    "    \"\"\"\n",
    "    weighted_X = np.zeros_like(X)\n",
    "    sum_row = np.sum(X, axis=1)\n",
    "    nb_documents = len(X)\n",
    "    d = np.count_nonzero(X, axis=0)\n",
    "    idf = np.log(nb_documents/d)\n",
    "    for i in trange(len(X)):\n",
    "        for j in range(len(X[0])):\n",
    "            if d[j]>0:\n",
    "                weighted_X[i,j] = (X[i,j]/sum_row[i])*idf[j]\n",
    "    return weighted_X\n",
    "\n",
    "weighted_matrice_coocurrence_document_mot_train = calculate_TFIDF(matrice_coocurrence_document_mot_train)\n",
    "weighted_matrice_coocurrence_document_mot_test = calculate_TFIDF(matrice_coocurrence_document_mot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eu-gIxbkAnKt"
   },
   "source": [
    "### 2.2 Matrice mot √ó mot et PPMI (*positive pointwise mutual information*)\n",
    "\n",
    "Vous devez calculer la m√©trique PPMI. Pour une matrice $m \\times n$ $X$ :\n",
    "\n",
    "\n",
    "$$\\textbf{colsum}(X, j) = \\sum_{i=1}^{m}X_{ij}$$\n",
    "\n",
    "$$\\textbf{sum}(X) = \\sum_{i=1}^{m}\\sum_{j=1}^{n} X_{ij}$$\n",
    "\n",
    "$$\\textbf{expected}(X, i, j) = \n",
    "\\frac{\n",
    "  \\textbf{rowsum}(X, i) \\cdot \\textbf{colsum}(X, j)\n",
    "}{\n",
    "  \\textbf{sum}(X)\n",
    "}$$\n",
    "\n",
    "\n",
    "$$\\textbf{pmi}(X, i, j) = \\log\\left(\\frac{X_{ij}}{\\textbf{expected}(X, i, j)}\\right)$$\n",
    "\n",
    "$$\\textbf{ppmi}(X, i, j) = \n",
    "\\begin{cases}\n",
    "\\textbf{pmi}(X, i, j) & \\textrm{if } \\textbf{pmi}(X, i, j) > 0 \\\\\n",
    "0 & \\textrm{otherwise}\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EXR_zFkuAnKu"
   },
   "source": [
    "**a)**\tA partir des textes du corpus d‚Äôentrainement (neg *et* pos), vous devez construire une matrice de co-occurrence mot √ó mot $M(w,w)$ qui contient les 5000 unigrammes les plus fr√©quents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovIWcX56PAGD"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def create_matrice_cooccurence_mot_mot(corpus, top_unigrams):\n",
    "    dico_top_unigrams = {}\n",
    "    cooccurrence_matrix = np.zeros((len(top_unigrams), len(top_unigrams)))\n",
    "    for i in range(len(top_unigrams)):\n",
    "        dico_top_unigrams[top_unigrams[i]] = i\n",
    "        \n",
    "    for sentence in tqdm(corpus):\n",
    "        for i in range(len(sentence)):\n",
    "            try:\n",
    "                index_current_word = dico_top_unigrams[sentence[i]]\n",
    "                for word in np.concatenate((sentence[max(0,i-5):i],sentence[i+1:i+6])):\n",
    "                    try:\n",
    "                        cooccurrence_matrix[index_current_word][dico_top_unigrams[word]]+=1\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "    return cooccurrence_matrix\n",
    "matrice_coocurrence_mot_mot_train = create_matrice_cooccurence_mot_mot(X_train, top_unigrams)\n",
    "matrice_coocurrence_mot_mot_test = create_matrice_cooccurence_mot_mot(X_test, top_unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-tfsnMCAnKy"
   },
   "source": [
    "**b)**\tVous devez cr√©er une fonction `calculate_PPMI` qui prend la matrice $M(w,w)$ et la transforme en une matrice $M‚Äô(w,w)$ avec les valeurs PPMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGD3gm6WAnKz"
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def calculate_PPMI(X):\n",
    "    \"\"\"\n",
    "    Pond√©ration de la matrice de co-occurrence mots x mots\n",
    "    :param X: list(list(int)), matrice de co-occurrence √† pond√©rer\n",
    "    :return: list(list(int)), matrice de co-occurrence pond√©r√© selon la m√©thode PPMI\n",
    "    \"\"\"\n",
    "    weighted_X = np.zeros_like(X)\n",
    "    sum_col = np.sum(X, axis=0)\n",
    "    sum_row = np.sum(X, axis=1)\n",
    "    sum_X = np.sum(X)\n",
    "    weights = np.transpose(np.dot(np.transpose([sum_col]),[sum_row]))/sum_X\n",
    "    for i in trange(len(X)):\n",
    "        for j in range(len(X[0])):\n",
    "            if weights[i,j]!=0 and X[i,j]>weights[i,j]:\n",
    "                weighted_X[i,j] = np.log(X[i,j]/weights[i,j])\n",
    "    return weighted_X\n",
    "\n",
    "weighted_matrice_coocurrence_mot_mot_train = calculate_PPMI(matrice_coocurrence_mot_mot_train)\n",
    "weighted_matrice_coocurrence_mot_mot_test = calculate_PPMI(matrice_coocurrence_mot_mot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eIRyC-k5AnK2"
   },
   "source": [
    "## 3. Mesures de similarit√©\n",
    "\n",
    "En utilisant le module [scipy.spatial.distance](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html),  d√©finissez des fonctions pour calculer les m√©triques suivantes :\n",
    "\n",
    "**Distance Euclidienne**\n",
    "\n",
    "La distance euclidienne entre deux vecteurs $u$ et $v$ de dimension $n$ est\n",
    "\n",
    "$$\\textbf{euclidean}(u, v) = \n",
    "\\sqrt{\\sum_{i=1}^{n}|u_{i} - v_{i}|^{2}}$$\n",
    "\n",
    "En deux dimensions, cela correspond √† la longueur de la ligne droite entre deux points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nJB3PicsAnK3"
   },
   "source": [
    "**a)** Impl√©mentez la fonction `get_euclidean_distance(v1 ,v2)` qui retourne la distance euclidienne entre les vecteurs v1 et v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ORfX0aJ7AnK4"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def get_euclidean_distance(v1, v2):\n",
    "    \"\"\"\n",
    "    Cettefonction calcule la distance euclidienne entre deux \n",
    "    vecteurs v1 et v2.\n",
    "    :param v1 : 1-D array, vecteur 1\n",
    "    :param v2 : 1-D array, vecteur 2\n",
    "    :return : float, distance entre les vecteurs v1 et v2\n",
    "    \"\"\"\n",
    "    \n",
    "    return euclidean(v1,v2)\n",
    "\n",
    "#Affichage d'un test :\n",
    "v1, v2 = ([0,1,2,3],[1,2,3,4])\n",
    "print('v1 : ', v1)\n",
    "print('v2 : ', v2)\n",
    "print(get_euclidean_distance(v1,v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lHVOg258AnK8"
   },
   "source": [
    "**Distance Cosinus**\n",
    "\n",
    "\n",
    "La distance cosinus entre deux vecteurs $u$ et $v$ de dimension $n$ s'√©crit :\n",
    "\n",
    "$$\\textbf{cosine}(u, v) = \n",
    "1 - \\frac{\\sum_{i=1}^{n} u_{i} \\cdot v_{i}}{\\|u\\|_{2} \\cdot \\|v\\|_{2}}$$\n",
    "\n",
    "Le terme de droite dans la soustraction mesure l'angle entre $u$ et $v$; on l'appelle la *similarit√© cosinus* entre $u$ et $v$.\n",
    "\n",
    "**b)** Impl√©mentez la fonction `get_cosinus_distance(v1, v2)` qui retourne la distance cosinus entre les vecteurs v1 et v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pcJQI1dhAnK9"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def get_cosinus_distance(v1, v2):\n",
    "    \"\"\"\n",
    "    Cettefonction calcule la distance cosinus entre deux \n",
    "    vecteurs v1 et v2.\n",
    "    :param v1 : 1-D array, vecteur 1\n",
    "    :param v2 : 1-D array, vecteur 2\n",
    "    :return : float, distance entre les vecteurs v1 et v2\n",
    "    \"\"\"\n",
    "    \n",
    "    return cosine(v1,v2)\n",
    "\n",
    "#Affichage d'un test :\n",
    "v1, v2 = ([0,1,2,3],[0,2,4,6])\n",
    "print('v1 : ', v1)\n",
    "print('v2 : ', v2)\n",
    "print(get_cosinus_distance(v1,v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HhGlTOkOAnLB"
   },
   "source": [
    "**c)** Impl√©mentez la fonction `get_most_similar_PPMI(word, metric, n)` qui prend un mot en entr√©e et une mesure de distance et qui retourne les n mots les plus similaires selon la mesure. Les mesures √† tester sont : la distance euclidienne et la distance cosinus implant√©es ci-dessus. Le vecteur du mot word doit √™tre extrait de la matrice $M‚Äô(w,w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5zetRaBVAnLC"
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def get_most_similar_PPMI(word, metric, n):\n",
    "    priority_queue = []\n",
    "    index_word = top_unigrams.index(word)\n",
    "    for i in range(len(weighted_matrice_coocurrence_mot_mot_train)):\n",
    "        if top_unigrams[i] != word:\n",
    "            heapq.heappush(priority_queue, (metric(weighted_matrice_coocurrence_mot_mot_train[index_word],weighted_matrice_coocurrence_mot_mot_train[i]), top_unigrams[i]))\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append(heapq.heappop(priority_queue)[1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UnzUMq5nAnLF"
   },
   "source": [
    "**d)** Trouvez les 5 mots les plus similaires au mot ¬´ bad ¬ª et affichez-les, pour chacune des deux distances. Commentez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZqt7qFQAnLG"
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "print(\"Les 5 mots les plus similaires √† 'bad' avec la distance euclidean :\")\n",
    "print(get_most_similar_PPMI('bad', get_euclidean_distance, n))\n",
    "print()\n",
    "print(\"Les 5 mots les plus similaires √† 'bad' avec la distance cosinus :\")\n",
    "print(get_most_similar_PPMI('bad', get_cosinus_distance, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ry-2_pt3AnLK"
   },
   "source": [
    "-> Commentez ici <-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFy81EDjAnLL"
   },
   "source": [
    "**e)** Impl√©mentez la fonction `get_most_similar_TFIDF(word, metric, n)` qui prend un mot en entr√©e et une mesure de distance et qui retourne les n mots les plus similaires selon la mesure. Les mesures √† tester sont : la distance euclidienne et la distance cosinus implant√©es ci-dessus. Le vecteur du mot word doit √™tre extrait de la matrice $M(d,w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7vRHdDqxAnLM"
   },
   "outputs": [],
   "source": [
    "def get_most_similar_TFIDF(word, metric, n):\n",
    "    priority_queue = []\n",
    "    index_word = top_unigrams.index(word)\n",
    "    for i in range(len(top_unigrams)):\n",
    "        if top_unigrams[i] != word:\n",
    "            heapq.heappush(priority_queue, (metric(weighted_matrice_coocurrence_document_mot_train[:,index_word],weighted_matrice_coocurrence_document_mot_train[:,i]), top_unigrams[i]))\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append(heapq.heappop(priority_queue)[1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awTSwa-7AnLS"
   },
   "source": [
    "**f)** Trouvez les 5 mots les plus similaires au mot ¬´ bad ¬ª et affichez-les, pour chacune des deux distances. Commentez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fKba94sDAnLT"
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "print(\"Les 5 mots les plus similaires √† 'bad' avec la distance euclidean :\")\n",
    "print(get_most_similar_TFIDF('bad', get_euclidean_distance, n))\n",
    "print()\n",
    "print(\"Les 5 mots les plus similaires √† 'bad' avec la distance cosinus :\")\n",
    "print(get_most_similar_TFIDF('bad', get_cosinus_distance, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlpvFibXAnLW"
   },
   "source": [
    "*-> Commentez ici <-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ClcQ0MhvAnLX"
   },
   "source": [
    "## 4. Classification de documents avec un mod√®le de langue\n",
    "\n",
    "En vous inspirant de [cet article](https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139), entra√Ænez deux mod√®les de langue $n$-gramme de caract√®re avec lissage de Laplace, l'un sur le corpus `pos`, l'autre sur le corpus `neg`. Puis, pour chaque document $D$, calculez sa probabilit√© selon vos deux mod√®les : $P(D \\mid \\textrm{pos})$ et $P(D \\mid \\textrm{neg})$.\n",
    "\n",
    "Vous pourrez alors pr√©dire sa classe $\\hat{c}_D \\in (\\textrm{pos}, \\textrm{neg})$ en prenant :\n",
    "\n",
    "$$\\hat{c}_D = \\begin{cases}\n",
    "\\textrm{pos} & \\textrm{si } P(D \\mid \\textrm{pos}) > P(D \\mid \\textrm{neg}) \\\\\n",
    "\\textrm{neg} & \\textrm{sinon}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4-y2hGSAnLY",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Training models\n",
    "import nltk\n",
    "import string\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#Ordre de notre model\n",
    "order = 4\n",
    "\n",
    "#Creation du vocabulaire \n",
    "vocab = nltk.lm.vocabulary.Vocabulary([chr(i) for i in range(ord('a'), ord('a')+27)]+[' '])\n",
    "\n",
    "#Separation des donn√©es 'pos' et 'neg' :\n",
    "data, n_grams, laplace = {},{},{}\n",
    "for label in ['pos', 'neg']:\n",
    "    print('Separation des donn√©es')\n",
    "    data[label] = np.array(X_train)[np.array(y_train)==label]\n",
    "    print('Transformation en minuscule et suppression de la ponctuation pour le set : '+label)\n",
    "    data[label] = [doc.translate(str.maketrans('', '', string.punctuation)).lower() for doc in tqdm(data[label])]\n",
    "    print('Collecte des ngrams pour le \"'+label+'\" set :')\n",
    "    n_grams[label] = [ngrams(pad_both_ends(document, order), order) for document in tqdm(data[label])]\n",
    "    laplace[label] = nltk.lm.models.Laplace(order, vocab)\n",
    "    print('Fitting du model laplace pour le set : '+label)\n",
    "    laplace[label].fit(n_grams[label])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f4ArMa_LJm8n"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def laplace_classifier(doc, order = order):\n",
    "    #Cleaning doc from punctuation and tokenizing it\n",
    "    tokens = [ch for ch in doc.translate(str.maketrans('', '', string.punctuation)).lower()]\n",
    "    \n",
    "    #Computing perplexity \n",
    "    perplexity_pos = 0\n",
    "    perplexity_neg = 0\n",
    "    n_grams = list(ngrams(tokens, order, pad_right=True, pad_left=True))\n",
    "    probs = {\"pos\":0,\"neg\":0}\n",
    "    for ngram in n_grams:\n",
    "        for label in probs.keys() :\n",
    "            probs[label] += math.log10(laplace[label].unmasked_score(ngram[-1], ngram[0:-1]))\n",
    "    \n",
    "    classe = [\"neg\",\"pos\"][probs[\"pos\"] > probs[\"neg\"]]\n",
    "    return classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FozNaxLUJpIe"
   },
   "outputs": [],
   "source": [
    "print(\"Testing Laplace Classifier :\")\n",
    "for i in range(5):\n",
    "    print(\"Test \",i+1,\" :\\n    Predicted classe : \" ,laplace_classifier(X_test[i]),\"\\t Target : \",y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEWiIkBLAnLc"
   },
   "source": [
    "## 5. Classification de documents avec sac de mots et Naive Bayes\n",
    "\n",
    "Ici, vous utiliserez l'algorithme Multinomial Naive Bayes (disponible dans [`sklearn.naive_bayes.MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)) pour classifier les documents. Vous utiliserez un mod√®le sac de mots (en anglais *bag of words*, ou BoW) avec TF-IDF pour repr√©senter vos documents.\n",
    "\n",
    "*Note :* vous avez d√©j√† construit la matrice TF-IDF √† la section 2.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0DnTz8nJ3A4"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "26fobjlAAnLd"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(weighted_matrice_coocurrence_document_mot_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HS9GHWK6JmJb"
   },
   "outputs": [],
   "source": [
    "print(\"Testing Multinomial Naive Bayes Classifier :\")\n",
    "for i in range(5):\n",
    "    print(\"Test \",i+1,\" :\\n    Predicted classe : \" ,\n",
    "          mnb_model.predict([weighted_matrice_coocurrence_document_mot_test[i]])[0],\n",
    "          \"\\t Target : \",y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IGY75wMfAnLg"
   },
   "source": [
    "## 6. Am√©liorations\n",
    "\n",
    "Ici, vous devez proposer une m√©thode d'am√©lioration pour le mod√®le pr√©c√©dent, la justifier et l'impl√©menter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fiXQqg_rAnLh"
   },
   "source": [
    "*-> √âcrivez vos explications ici <-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2RjWD-a_AnLl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bs1YBBBJAnLo"
   },
   "source": [
    "## 7. √âvaluation\n",
    "\n",
    "√âvaluation des mod√®les des sections 4, 5, 6 sur les donn√©es de test. On attend les m√©triques suivantes : *accuracy*, et pour chaque classe pr√©cision, rappel, score F1. Vous pourrez utiliser le module [`sklearn.metrics`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mVwJyGoBAnLp"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "predictions = {}\n",
    "predictions[\"Vanilla Laplace\"] = []\n",
    "predictions[\"MNB\"] = []\n",
    "predictions[\"Laplace Am√©lior√©\"] = []\n",
    "\n",
    "for model_name in predictions.keys():\n",
    "    print(\"Test du classifieur \",model_name,\" : \")\n",
    "    print(sklearn.metrics.classification_report(y_test, predictions[model_name])) \n",
    "    print(sklearn.metrics.confusion_matrix(y_test, predictions[model_name]), columns=[\"neg\", \"pos\"], index=[\"neg\", \"pos\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CXMk-CPYAnLs"
   },
   "source": [
    "Commentez vos r√©sultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yBPtu16JAnLt"
   },
   "source": [
    "*-> Commentez ici vos r√©sultats <-*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "inf8460_tp2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
